{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10TrkrLAej16spZ6zrIGyVV3eEKAZ95hK",
      "authorship_tag": "ABX9TyParFp5Gw2gipgx/occqiCM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuellerLeonard/Skip-Gram_fastText/blob/master/fastTextV.1.3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yuhFPWjyQGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "e3fa052a-f75a-4d9c-8397-9e819416e948"
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install keras\n",
        "!pip install requests\n",
        "!pip install fasttext"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (49.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.1.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj3sZzNZyqzG",
        "colab_type": "text"
      },
      "source": [
        "Needed classes for imports:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXRHeo3Cy7XR",
        "colab_type": "text"
      },
      "source": [
        "cleanup.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0hVox5sytjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "68b39745-df4b-484f-ac22-ee14801ad026"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(tweets: List[str], lang: str = 'english') -> List[str]:\n",
        "    \"\"\"\n",
        "    performes stemming and other \"cleanup\"\n",
        "    :param tweets:\n",
        "    :param lang:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    data: List[str] = tweets\n",
        "    cleaned_text = []\n",
        "    stops = set(stopwords.words(lang))\n",
        "\n",
        "    # no ! and .\n",
        "    table = str.maketrans(dict.fromkeys(\"\"\"\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\"\"\"))\n",
        "\n",
        "    for text in data:\n",
        "        text = re.sub(r'https?://[^\\s]*', ' tokenlink ', text)\n",
        "        text = re.sub(r'\\.\\.\\.', ' tokendotdotdot ', text)\n",
        "        text = re.sub(r'xD', ' tokenxd ', text)\n",
        "        text = re.sub(r':\\)', ' tokenxbrackethappy ', text)\n",
        "        text = re.sub(r':\\(', ' tokenxbracketsad ', text)\n",
        "        text = re.sub(r':-\\)', ' tokennosehappy ', text)\n",
        "        text = re.sub(r':-\\(', ' tokennosesad ', text)\n",
        "        text = re.sub(r':D', ' tokenxcheer ', text)\n",
        "        text = re.sub(r':-S', ' tokens ', text)\n",
        "\n",
        "\n",
        "        ## Convert words to lower case and split them\n",
        "        text = text.lower().split()\n",
        "\n",
        "        text = \" \".join(text)\n",
        "\n",
        "        # Clean the text\n",
        "        text = re.sub(r\"(?<!\\.)\\.(?!\\.)\", \" \", text)\n",
        "        text = re.sub(r\"[^A-Za-z0-9^,!./'+-=]\", \" \", text)\n",
        "        text = re.sub(r\"what's\", \"what is \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"i'm\", \"i am \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "        text = re.sub(r\",\", \" \", text)\n",
        "        text = re.sub(r\"\\.\", \" \", text)\n",
        "        text = re.sub(r\"!\", \" ! \", text)\n",
        "        text = re.sub(r\"/\", \" \", text)\n",
        "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "        text = re.sub(r\"\\+\", \" + \", text)\n",
        "        text = re.sub(r\"-\", \" - \", text)\n",
        "        text = re.sub(r\"=\", \" = \", text)\n",
        "        text = re.sub(r\"'\", \" \", text)\n",
        "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "        text = re.sub(r\":\", \" : \", text)\n",
        "        text = re.sub(r\" e g \", \" eg \", text)\n",
        "        text = re.sub(r\" b g \", \" bg \", text)\n",
        "        text = re.sub(r\" u s \", \" american \", text)\n",
        "        text = re.sub(r\"\\0s\", \"0\", text)\n",
        "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "        text = re.sub(r\"e - mail\", \"email\", text)\n",
        "        text = re.sub(r\"j k\", \"jk\", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "        ## Remove puncuation\n",
        "        text = text.translate(table)\n",
        "\n",
        "        text = text.split()\n",
        "\n",
        "        ## Remove stop words[^\\s][^\\s]\n",
        "        #text = [w for w in text if not w in stops]\n",
        "\n",
        "        stemmer = SnowballStemmer(lang)\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "        cleaned_text.append(text)\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsPd_IYrzGMo",
        "colab_type": "text"
      },
      "source": [
        "NBT.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzwORX2szK88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "8862e80f-cbf5-4d86-bb0b-516f2ef51697"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "class NBT():\n",
        "    \"\"\"\n",
        "    Nyström Basis Transfer Service Class\n",
        "    Published in:\n",
        "    Christoph Raab, Frank-Michael Schleif,\n",
        "    Transfer learning extensions for the probabilistic classification vector machine,Neurocomputing,2019,\n",
        "    https://doi.org/10.1016/j.neucom.2019.09.104.\n",
        "    Functions\n",
        "    ----------\n",
        "    nys_basis_transfer: Transfer Basis from Target to Source Domain.\n",
        "    data_augmentation: Augmentation of data by removing or upsampling of source data\n",
        "    Examples\n",
        "    --------\n",
        "    >>> #Imports\n",
        "    >>> import os\n",
        "    >>> import scipy.io as sio\n",
        "    >>> from sklearn.svm import SVC\n",
        "    >>> os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
        "    >>> os.chdir(os.path.join(\"datasets\",\"domain_adaptation\",\"features\",\"OfficeCaltech\"))\n",
        "    >>> amazon = sio.loadmat(\"amazon_SURF_L10.mat\")\n",
        "    >>> X = preprocessing.scale(np.asarray(amazon[\"fts\"]))\n",
        "    >>> Yt = np.asarray(amazon[\"labels\"])\n",
        "    >>>\n",
        "    >>> dslr = sio.loadmat(\"dslr_SURF_L10.mat\")\n",
        "    >>>\n",
        "    >>> Z = preprocessing.scale(np.asarray(dslr[\"fts\"]))\n",
        "    >>> Ys = np.asarray(dslr[\"labels\"])\n",
        "    >>>\n",
        "    >>> clf = SVC(gamma=1,C=10)\n",
        "    >>> clf.fit(Z,Ys)\n",
        "    >>> print(\"SVM: \"+str(clf.score(X,Yt)))\n",
        "    >>>\n",
        "    >>> nbt = NBT()\n",
        "    >>> Ys,Z = nbt.data_augmentation(Z,X.shape[0],Ys)\n",
        "    >>> X,Z = nbt.nys_basis_transfer(X,Z,Ys.flatten(),landmarks=100)\n",
        "    >>> clf = SVC(gamma=1,C=10)\n",
        "    >>> clf.fit(Z,Ys)\n",
        "    >>> print(\"SVM + NBT: \"+str(clf.score(X,Yt)))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,landmarks=10):\n",
        "        self.landmarks = landmarks\n",
        "        pass\n",
        "\n",
        "    def nys_basis_transfer(self,X,Z,Ys=None):\n",
        "        \"\"\"\n",
        "        Nyström Basis Transfer\n",
        "        Transfers Basis of X to Z obtained by Nyström SVD\n",
        "        Implicit dimensionality reduction\n",
        "        Applications in domain adaptation or transfer learning\n",
        "        Parameters.\n",
        "        Note target,source are order sensitiv.\n",
        "        ----------\n",
        "        X : Target Matrix, where classifier is trained on\n",
        "        Z : Source Matrix, where classifier is trained on\n",
        "        Ys: Source data label, if none, classwise sampling is not applied.\n",
        "        landmarks : Positive integer as number of landmarks\n",
        "        Returns\n",
        "        ----------\n",
        "        X : Reduced Target Matrix\n",
        "        Z : Reduced approximated Source Matrix\n",
        "        \"\"\"\n",
        "        if type(X) is not np.ndarray or type(Z) is not np.ndarray:\n",
        "            raise ValueError(\"Numpy Arrays must be given!\")\n",
        "        if type(self.landmarks ) is not int or self.landmarks  < 1:\n",
        "            raise ValueError(\"Positive integer number must given!\")\n",
        "        landmarks = np.min([X.shape[0]]+[Z.shape[0]]+[self.landmarks ])\n",
        "        max_idx = np.min(list(X.shape)+list(Z.shape))\n",
        "        idx = np.random.randint(0,max_idx-1,landmarks)\n",
        "        A = X[np.ix_(idx,idx)]\n",
        "        # B = X[0:landmarks,landmarks:]\n",
        "        F = X[landmarks:,0:landmarks]\n",
        "        #C = X[landmarks:,landmarks:]\n",
        "        U, S, H = np.linalg.svd(A, full_matrices=True)\n",
        "        S = np.diag(S)\n",
        "\n",
        "        U_k = np.concatenate([U,(F @H )@np.linalg.pinv(S)])\n",
        "        #V_k = np.concatenate([H, np.matmul(np.matmul(B.T,U),np.linalg.pinv(S))])\n",
        "        X = U_k @S\n",
        "\n",
        "        if type(Ys) is np.ndarray:\n",
        "            A = self.classwise_sampling(Z,Ys,landmarks)\n",
        "        else:\n",
        "            A = Z[np.ix_(idx,idx)]\n",
        "\n",
        "        D = np.linalg.svd(A, full_matrices=True,compute_uv=False)\n",
        "        Z = U_k @ np.diag(D)\n",
        "        return preprocessing.scale(X),preprocessing.scale(Z)\n",
        "\n",
        "    def classwise_sampling(self,X,Y,n_landmarks):\n",
        "\n",
        "        A = []\n",
        "        classes = np.unique(Y)\n",
        "        c_classes = classes.size\n",
        "        samples_per_class = int(n_landmarks / c_classes)\n",
        "        for c in classes:\n",
        "            class_data = X[np.where(c == Y)]\n",
        "\n",
        "            if samples_per_class > class_data.shape[0]:\n",
        "                A = A+list(class_data)\n",
        "            else:\n",
        "                A = A+list(class_data[np.random.randint(0,class_data.shape[0],samples_per_class)])\n",
        "\n",
        "        return np.array(A)\n",
        "\n",
        "    def basis_transfer(self,X,Z):\n",
        "        \"\"\"\n",
        "         Basis Transfer\n",
        "        Transfers Basis of X to Z obtained by Nyström SVD\n",
        "        Applications in domain adaptation or transfer learning\n",
        "        Parameters.\n",
        "        Note target,source are order sensitiv.\n",
        "        ----------\n",
        "        X : Target Matrix, where classifier is trained on\n",
        "        Z : Source Matrix, where classifier is trained on\n",
        "        Returns\n",
        "        ----------\n",
        "        Z : Transferred Source Matrix\n",
        "        \"\"\"\n",
        "        L,S,R = np.linalg.svd(X,full_matrices=False);\n",
        "        D = np.linalg.svd(Z,compute_uv=False,full_matrices=False)\n",
        "        return L @ np.diag(D) @ R\n",
        "\n",
        "    def fit_predict(self, Xs, Ys, Xt, Yt):\n",
        "        '''\n",
        "        Fit and use 1NN to classify\n",
        "        :param Xs: ns * n_feature, source feature\n",
        "        :param Ys: ns * 1, source label\n",
        "        :param Xt: nt * n_feature, target feature\n",
        "        :param Yt: nt * 1, target label\n",
        "        :return: Accuracy, predicted labels of target domain, and G\n",
        "        '''\n",
        "        clf = KNeighborsClassifier(n_neighbors=1)\n",
        "        clf.fit(Xs, Ys.ravel())\n",
        "        y_pred = clf.predict(Xt)\n",
        "        acc = np.mean(y_pred == Yt.ravel())\n",
        "        return acc, y_pred\n",
        "\n",
        "\n",
        "    def data_augmentation(self,Z,required_size,Y):\n",
        "        \"\"\"\n",
        "        Data Augmentation\n",
        "        Upsampling if Z smaller as required_size via multivariate gaussian mixture\n",
        "        Downsampling if Z greater as required_size via uniform removal\n",
        "        Note both are class-wise with goal to harmonize class counts\n",
        "        ----------\n",
        "        Z : Matrix, where classifier is trained on\n",
        "        required_size : Size to which Z is reduced or extended\n",
        "        Y : Label vector, which is reduced or extended like Z\n",
        "        Returns\n",
        "        ----------\n",
        "        X : Augmented Z\n",
        "        Z : Augmented Y\n",
        "        \"\"\"\n",
        "        if type(Z) is not np.ndarray or type(required_size) is not int or type(Y) is not np.ndarray:\n",
        "            raise ValueError(\"Numpy Arrays must be given!\")\n",
        "        if Z.shape[0] == required_size:\n",
        "            return Y,Z\n",
        "        \n",
        "        _, idx = np.unique(Y, return_index=True)\n",
        "        C = Y[np.sort(idx)].flatten().tolist()\n",
        "        size_c = len(C)\n",
        "        if Z.shape[0] < required_size:\n",
        "            print(\"Source smaller target\")\n",
        "            data = np.empty((0,Z.shape[1]))\n",
        "            label = np.empty((0,1))\n",
        "            diff = required_size - Z.shape[0]\n",
        "            sample_size = int(np.floor(diff/size_c))\n",
        "            for c in C:\n",
        "                #indexes = np.where(Y[Y==c])\n",
        "                indexes =  np.where(Y==c)\n",
        "                class_data = Z[indexes,:][0]\n",
        "                m = np.mean(class_data,0) \n",
        "                sd = np.var(class_data,0)\n",
        "                sample_size = sample_size if c !=C[-1] else sample_size+np.mod(diff,size_c)\n",
        "                augmentation_data =np.vstack([np.random.normal(m, sd, size=len(m)) for i in range(sample_size)])\n",
        "                data =np.concatenate([data,class_data,augmentation_data])\n",
        "                label = np.concatenate([label,np.ones((class_data.shape[0]+sample_size,1))*c])\n",
        "            \n",
        "        if Z.shape[0] > required_size:\n",
        "            print(\"Source greater target\")\n",
        "            data = np.empty((0,Z.shape[1]))\n",
        "            label = np.empty((0,1))\n",
        "            sample_size = int(np.floor(required_size/size_c))\n",
        "            for c in C:\n",
        "                indexes = np.where(Y[Y==c])[0]\n",
        "                class_data = Z[indexes,:]\n",
        "                if len(indexes) > sample_size:\n",
        "                    sample_size = sample_size if c !=C[-1] else np.abs(data.shape[0]-required_size)\n",
        "                    y = np.random.choice(class_data.shape[0],sample_size)\n",
        "                    class_data = class_data[y,:]\n",
        "                data =np.concatenate([data,class_data])\n",
        "                label = np.concatenate([label,np.ones((class_data.shape[0],1))*c])\n",
        "        Z = data\n",
        "        Y = label\n",
        "        return Y,Z\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    import os\n",
        "    import scipy.io as sio\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    # Load and preprocessing of data. Note normalization to N(0,1) is necessary.\n",
        "    # not needed files were directly accessed\n",
        "    # os.chdir(\"datasets/domain_adaptation/OfficeCaltech/features/Surf\")\n",
        "    amazon = sio.loadmat(\"/content/drive/My Drive/googleColabFiles/amazon_SURF_L10.mat\")\n",
        "    X = preprocessing.scale(np.asarray(amazon[\"fts\"]))\n",
        "    Yt = np.asarray(amazon[\"labels\"])\n",
        "\n",
        "    dslr = sio.loadmat(\"/content/drive/My Drive/googleColabFiles/dslr_SURF_L10.mat\")\n",
        "\n",
        "    Z = preprocessing.scale(np.asarray(dslr[\"fts\"]))\n",
        "    Ys = np.asarray(dslr[\"labels\"])\n",
        "\n",
        "    # Applying SVM without transfer learning. Accuracy should be about 10%\n",
        "    clf = SVC(gamma=1,C=10)\n",
        "    clf.fit(Z,Ys)\n",
        "    print(\"SVM without transfer \"+str(clf.score(X,Yt)))\n",
        "\n",
        "    # Beginning of NBT. Accuracy of SVM + NBT should be about 90%\n",
        "    nbt = NBT(landmarks=100)\n",
        "    # Data augmentation is necessary if Z and X have different shapes.\n",
        "    Ys,Z = nbt.data_augmentation(Z,X.shape[0],Ys)\n",
        "    X,Z = nbt.nys_basis_transfer(X,Z,Ys.flatten())\n",
        "    clf = SVC(gamma=1,C=10)\n",
        "    clf.fit(Z,Ys)\n",
        "    print(\"SVM + NBT: \"+str(clf.score(X,Yt)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM without transfer 0.10438413361169102\n",
            "Source smaller target\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM + NBT: 0.9311064718162839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWHOlS5JzbZl",
        "colab_type": "text"
      },
      "source": [
        "sentqs_preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvlJYqlHzeJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "48494c10-2ce4-4b5d-b9ab-b90c663d25b2"
      },
      "source": [
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "# import cleanup --> already in notebook\n",
        "import keras\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, pad_sequences\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn import decomposition\n",
        "#from keras_preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import make_sampling_table\n",
        "from numpy import asarray, zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling3D, GlobalMaxPooling2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, Conv2D, MaxPooling3D, Embedding, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from scipy import spatial\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "# fastText\n",
        "#import fasttext.util\n",
        "import fasttext\n",
        "from fasttext import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from io import StringIO\n",
        "import math\n",
        "from gensim.models import FastText\n",
        "# 1DConv Net for comparison\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Conv2D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\n",
        "def load_data_run_classification():\n",
        "  # 'data/....npz'\n",
        "    data = np.load('/content/drive/My Drive/googleColabFiles/sentqs_dataset.npz')\n",
        "    Xs = data[\"arr_0\"]\n",
        "    Ys = data[\"arr_1\"]\n",
        "    Xt = data[\"arr_2\"]\n",
        "    Yt = data[\"arr_3\"]\n",
        "    print(\"Classification Task Test \\n\")\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Xs, Ys)\n",
        "    print(clf.score(Xt, Yt))\n",
        "\n",
        "def create_tfidf(sen,min_df=10,max_df=100):\n",
        "    print(\"Create TF-IDF\\n\")\n",
        "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
        "    X = vectorizer.fit_transform(sen)\n",
        "    return X.toarray()\n",
        "\n",
        "def save_dataset(X, y, prefix =\"\"):\n",
        "    print(\"Save Dataset \\n\")\n",
        "    y = np.array(y)[:, None]\n",
        "    dataset = np.concatenate([y,X], axis=1)\n",
        "\n",
        "    # data/\n",
        "    np.save(\"sentqs_da_\"+str(prefix)+\".npy\",dataset)\n",
        "    return dataset\n",
        "\n",
        "def seperate_tweets(data,hashtags,sentiment):\n",
        "    print(\"Seperate Tweets \\n\")\n",
        "    labels = []\n",
        "    tweets = []\n",
        "    sentiment_new = []\n",
        "    for t,s in zip(data,sentiment):\n",
        "        for h in hashtags:\n",
        "            t = t.lower()\n",
        "            h = \"#\" + h.lower()\n",
        "            if h in t:\n",
        "                labels.append(h)\n",
        "                tweets.append(t.replace(h,\" \"))\n",
        "                sentiment_new.append(s)\n",
        "                break\n",
        "\n",
        "    return labels,tweets,sentiment_new\n",
        "\n",
        "def generate_data(corpus, window_size, V):\n",
        "    for words in corpus:\n",
        "        couples, labels = skipgrams(words, V, window_size, negative_samples=1, shuffle=True,sampling_table=make_sampling_table(V, sampling_factor=1e-05))\n",
        "        if couples:\n",
        "            X, y = zip(*couples)\n",
        "            X = np_utils.to_categorical(X, V)\n",
        "            y = np_utils.to_categorical(y, V)\n",
        "            yield X, y\n",
        "\n",
        "def get_glove_embedding_matrix(texts, dim=200):\n",
        "    # data/\n",
        "    if os.path.isfile(\"sentqs_glove_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_glove_embedding.npz\")\n",
        "        print('Loaded Glove embedding.')\n",
        "        return loaded_embedding['embedding']\n",
        "    else:\n",
        "        # first, build index mapping words in the embeddings set\n",
        "        # to their embedding vector\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "        word_index = tokenizer.word_index\n",
        "\n",
        "        print('Indexing word vectors.')\n",
        "\n",
        "        embeddings_index = {}\n",
        "        # data/\n",
        "        # evtl. größeren Datensatz wenn möglich verwenden\n",
        "        with open('/content/drive/My Drive/googleColabFiles/dataset/glove.twitter.27B.200d.txt', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "                embeddings_index[word] = coefs\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "        print('Preparing embedding matrix.')\n",
        "\n",
        "        # prepare embedding matrix\n",
        "        num_words = len(word_index) + 1\n",
        "        embedding_matrix = np.zeros((num_words, dim))\n",
        "        counter = 0\n",
        "        for word, i in word_index.items():\n",
        "            # if i >= MAX_NUM_WORDS:\n",
        "            #    counter +=1\n",
        "            #    continue\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_glove_embedding.npz\", embedding=embedding_matrix)\n",
        "        return embedding_matrix\n",
        "\n",
        "def get_fastText_embedding_matrix(texts, BASE_DIR = '', MAX_SEQUENCE_LENGTH = 1000, EMBEDDING_DIM = 200):\n",
        "# dataset dimension must be 200\n",
        "    FT_MODEL = \"/content/drive/My Drive/googleColabFiles/dataset/cc.en200.bin\"\n",
        "    \n",
        "    #code for reducing dimensions of file cc.en200.bin, path needs to be adjusted\n",
        "    #FT_MODEL =fasttext.load_model('/content/drive/My Drive/googleColabFiles/dataset/cc.en.300.bin')\n",
        "    #print(\"loading model\")\n",
        "    #fasttext.util.reduce_model(FT_MODEL, EMBEDDING_DIM)\n",
        "    #print(\"dimensions reduced\")\n",
        "\n",
        "    #FT_MODEL.get_dimension()\n",
        "\n",
        "    if os.path.isfile(\"sentqs_fastText_embedding.npz\"):\n",
        "      saved_embedding = np.load(\"sentqs_fastText_embedding.npz\")\n",
        "      print('loaded pre_trained_fastText embedding.')\n",
        "      return saved_embedding['embedding']\n",
        "\n",
        "    else:\n",
        "      #obtain pretrained vectors:\n",
        "      print('Indexing word vectors.')\n",
        "\n",
        "      filename, file_extension = os.path.splitext(FT_MODEL)\n",
        "\n",
        "      if file_extension == '.vec':\n",
        "          embeddings_index = {}\n",
        "          with open(os.path.join(FT_MODEL)) as f:\n",
        "              for line in f:\n",
        "                  values = line.split()\n",
        "                  word = values[0]\n",
        "                  coefs = np.asarray(values[1:], dtype='float32')\n",
        "                  embeddings_index[word] = coefs\n",
        "            \n",
        "          print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "      # use pretrained model on tweets:\n",
        "      print('Found %s texts.' % len(texts))\n",
        "      # finally, vectorize the text samples into a 2D integer tensor\n",
        "      # num_words = size of vocab\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(texts)\n",
        "      sequences = tokenizer.texts_to_sequences(texts)\n",
        "      word_index = tokenizer.word_index\n",
        "      print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "      print('Preparing embedding matrix.')\n",
        "\n",
        "      # load the fasttext model\n",
        "      f = load_model(FT_MODEL)\n",
        "\n",
        "      # prepare embedding matrix\n",
        "      num_words = len(word_index) + 1\n",
        "      emb_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "      for word, i in word_index.items():\n",
        "        # if i >= MAX_NUM_WORDS:\n",
        "          # continue\n",
        "        # embedding_vector = embeddings_index.get(word)\n",
        "          embedding_vector = f.get_word_vector(word)\n",
        "          if embedding_vector is not None:\n",
        "              # words not found in embedding index will be all-zeros.\n",
        "              emb_matrix[i] = embedding_vector\n",
        "\n",
        "      np.savez_compressed(\"sentqs_fastText_embedding.npz\", embedding = emb_matrix)\n",
        "      return emb_matrix\n",
        "\n",
        "def get_fastText_sentence_embedding_matrix(text, dim=200):\n",
        "\n",
        "    if os.path.isfile(\"sentqs_fastText_sentence_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_fastText_sentence_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded fastText_sentence embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        text = [''.join(x) for x in text]\n",
        "\n",
        "        #create .txt file from cleaned_tweets\n",
        "        df = pd.DataFrame(text)\n",
        "        df.to_csv(r'text.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "        #train unsupervised model with skipgram\n",
        "        modelft = fasttext.train_unsupervised('text.txt', \"skipgram\", dim=dim)\n",
        "        #Model can be saved:\n",
        "        #modelft.save_model(\"textmodel.txt\")\n",
        "\n",
        "        #t = Tokenizer()\n",
        "        #t.fit_on_texts(text)\n",
        "\n",
        "        #get each sentence vector from fastText Model and create a matrix\n",
        "        i = 0\n",
        "        sent_matrix = np.zeros((len(text), dim))\n",
        "        for sentence in text:\n",
        "          sentence_vector = modelft.get_sentence_vector(sentence)\n",
        "          if sentence_vector is not None:\n",
        "            sent_matrix[i] = sentence_vector\n",
        "            i = i + 1\n",
        "            if i == len(text):\n",
        "              break\n",
        "\n",
        "        #print(\"sentence_matrix: \", sent_matrix)\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_fastText_sentence_embedding\", embedding=sent_matrix)\n",
        "        return sent_matrix   \n",
        "\n",
        "# epochs 100 to 10\n",
        "# batch_size, epochs depends on how much input data there is\n",
        "def get_skipgram_sentence_embedding_matrix(text, dim=200, batch_size=64, window_size=5, epochs = 1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_sentence_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_sentence_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        text = [''.join(x) for x in text]\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "        corpus = t.texts_to_sequences(text)\n",
        "        #print(corpus)\n",
        "        V = len(t.word_index)\n",
        "        step_size = len(corpus) // batch_size\n",
        "        model = Sequential()\n",
        "        model.add(Dense(dim, input_dim=V, activation=\"softmax\"))\n",
        "        model.add(Dense(V, input_dim=dim, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "        model.summary()\n",
        "\n",
        "        model.fit(generate_data(corpus, window_size, V), epochs=epochs, steps_per_epoch=step_size)\n",
        "        # model.save(\"data/sentqs_full_skigram_arc.h5\")\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        print(\"step 1\")\n",
        "        enc = mlb.fit_transform(corpus)\n",
        "        print(\"step 2\")\n",
        "        emb = enc @ model.get_weights()[0]\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_sentence_embedding\", embedding=emb)\n",
        "        return emb\n",
        "\n",
        "def  get_skipgram_gensim_embedding_matrix(text, dim = 200, window_size=5, min_word_occurance=1, epochs=1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_gensim_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_gensim_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        x = [row.split(' ') for row in text]\n",
        "        model = Word2Vec(x, size=dim, window=window_size, min_count=min_word_occurance, workers=4, sg=1) #sg = 1: use skipgram\n",
        "\n",
        "        words = model.wv.vocab.keys()\n",
        "        vocab_size = len(words)\n",
        "        print(\"Vocab size\", vocab_size)\n",
        "\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "\n",
        "        # total vocabulary size plus 0 for unknown words\n",
        "        # vocab_size = len(vocab) + 1\n",
        "        # define weight matrix dimensions with all 0\n",
        "        weight_matrix = zeros((vocab_size, dim))\n",
        "        # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "        for word, i in t.word_index.items():\n",
        "            if i > vocab_size: break\n",
        "            if word in model.wv.vocab.keys():\n",
        "                weight_matrix[i] = model.wv[word]\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_gensim_embedding\", embedding=weight_matrix)\n",
        "        return weight_matrix\n",
        "\n",
        "def get_fastText_gensim_embedding_matrix(text, dim = 200, window_size=3, min_word_occurance=1, epochs=10):\n",
        "    if os.path.isfile(\"sentqs_fastText_gensim_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_fastText_gensim_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded fastText_gensim embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        x = [row.split(' ') for row in text]\n",
        "        modelft = FastText(size=dim, window=window_size, min_count=min_word_occurance) \n",
        "        modelft.build_vocab(sentences=x)\n",
        "        modelft.train(sentences=x, total_examples=len(x), epochs=epochs) #train\n",
        "\n",
        "        words = modelft.wv.vocab.keys()\n",
        "        vocab_size = len(words)\n",
        "        print(\"Vocab size\", vocab_size)\n",
        "\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "\n",
        "        # total vocabulary size plus 0 for unknown words\n",
        "        # vocab_size = len(vocab) + 1\n",
        "        # define weight matrix dimensions with all 0\n",
        "        ft_weight_matrix = zeros((vocab_size, dim))\n",
        "        # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "        for word, i in t.word_index.items():\n",
        "            if i > vocab_size: break\n",
        "            if word in modelft.wv.vocab.keys():\n",
        "                ft_weight_matrix[i] = modelft.wv[word]\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_fastText_gensim_embedding\", embedding=ft_weight_matrix)\n",
        "        return ft_weight_matrix\n",
        "\n",
        "\n",
        "def print_results(N, p, r):\n",
        "    print(\"N\\t\" + str(N))\n",
        "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
        "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
        "\n",
        "\n",
        "def text_classification_ft(text, sentiment, train_size=0.8, epochs=25, learningrate=1.0, word_n_grams=2, dim=50, bucket=200000, loss='hs'):\n",
        "    #sentiment: 2.0 = negative, 1.0 = positive, 0.0 = neutral\n",
        "    #supervised fastText only for Tweets.csv\n",
        "    #does not have to be Tokenized\n",
        "    \n",
        "    #converting float values of sentiment to string values in a new column\n",
        "    score = [sentiment.size]\n",
        "    for x in sentiment:\n",
        "        if x == 0.0:\n",
        "          score.append(\"neutral\")  \n",
        "        elif x == 1.0:\n",
        "          score.append(\"positive\")\n",
        "        elif x == 2.0:\n",
        "          score.append(\"negative\")\n",
        "        \n",
        "    score.pop(0)\n",
        "\n",
        "    print(len(score))\n",
        "    print(score) \n",
        "\n",
        "    #Preprocessing\n",
        "    #combining individual columns to a table\n",
        "    texts = [''.join(x) for x in text]\n",
        "    df = pd.DataFrame(texts)\n",
        "    dfs = pd.DataFrame(sentiment)\n",
        "    dfsc = pd.DataFrame(score)\n",
        "    combined = pd.concat([dfs, df, dfsc], axis=1)\n",
        "    combined.columns = ['sentiment', 'text', 'score']\n",
        "    combined.to_csv('tweets.csv', index=False)\n",
        "    tweets = pd.read_csv(\"tweets.csv\")\n",
        "    \n",
        "    print(tweets.head())\n",
        "\n",
        "    #formatting the data-table so that fastText can use it\n",
        "    col = ['score', 'text']\n",
        "\n",
        "    tweets = tweets[col]\n",
        "    tweets['score']=['__label__'+ s for s in tweets['score']]\n",
        "    tweets['text']= tweets['text'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "    tweets.to_csv(r'tweets_updated.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "    print(\"updated dataframe: \")\n",
        "    print(tweets.head())\n",
        "    train_data = math.ceil(len(texts) * train_size)\n",
        "    test_data = math.floor(len(texts) * (1-train_size))\n",
        "  \n",
        "    #split data into head and tail for train and test\n",
        "    print(\"splitting data into training and test\", train_data, \",\", test_data)\n",
        "    tweets_train = tweets.head(train_data)\n",
        "    tweets_train.to_csv(r'tweets_train.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "    tweets_test = tweets.tail(test_data)\n",
        "    tweets_test.to_csv(r'tweets_test.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "    print(\"train fastText Model supervised with the training data: \")\n",
        "    # Hyperparameters can be optimized, using the Standard ones\n",
        "    modelft = fasttext.train_supervised(input=\"tweets_train.txt\")#, lr=learningrate, epoch=epochs, wordNgrams=word_n_grams, bucket=bucket, dim=dim, loss=loss)\n",
        "    print(\"words: \", modelft.words)\n",
        "    print(\"labels: \", modelft.labels)\n",
        "    print(\"Precision and Recall on test data: \")\n",
        "    print_results(*modelft.test(\"tweets_test.txt\"))\n",
        "\n",
        "    print(\"predict on a sentence and return the probability of the labels: \")\n",
        "    sentence = \"How much of them have covered as of today? What is the number of shares still shorted? Thank you man! How far can this wave roll?\"\n",
        "    print(\"sentence: \", sentence)\n",
        "    print(modelft.predict(sentence, k=3))\n",
        "\n",
        "def generate_embedding_model(text, y,source_idx,target_idx,batch_size=32, epochs = 20, save = True, dim = 200, val_split=0.2,model_size=\"2D\"):\n",
        "    # Preprocessing\n",
        "    #MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    MAX_SEQUENCE_LENGTH = 335\n",
        "    texts = [''.join(x) for x in text]\n",
        "    # finally, vectorize the text samples into a 2D integer tensor\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    labels = to_categorical(np.asarray(y))\n",
        "    print('Shape of data tensor:', data.shape)\n",
        "    print('Shape of label tensor:', labels.shape)\n",
        "    num_words = len(word_index) + 1\n",
        "    #for sentence classification\n",
        "    num_sentences = len(text)\n",
        "\n",
        "    # split the data into a training set and a validation set\n",
        "    indices = np.arange(data.shape[0])\n",
        "    print(\"indices: \", indices.shape)\n",
        "    np.random.shuffle(indices)\n",
        "    data = data[indices]\n",
        "    print(\"data: \", data.shape)\n",
        "    #print(\"data: \", data)\n",
        "    labels = labels[indices]\n",
        "    num_validation_samples = int(val_split * data.shape[0])\n",
        "\n",
        "    x_train = data[source_idx]\n",
        "    y_train = labels[source_idx]\n",
        "    x_val = data[target_idx]\n",
        "    y_val = labels[target_idx]\n",
        "\n",
        "    #for sentence classification:\n",
        "    #1D array, mapping one label to each sentence, like word classification only for sentences --> very clunky, but how i understood it\n",
        "    #each sentence will get a vector from the sentence_embedding_matrix\n",
        "    #accuracy around 60%\n",
        "\n",
        "    number_sentences = len(text)\n",
        "    matrix = np.arange(number_sentences)\n",
        "    matrix = matrix[indices]\n",
        "    w_train = matrix[source_idx]\n",
        "    v_train = labels[source_idx]\n",
        "    w_val = matrix[target_idx]\n",
        "    v_val = labels[target_idx]\n",
        "\n",
        "    # not sure\n",
        "    emb = get_skipgram_gensim_embedding_matrix(text, epochs=1)\n",
        "    emb = np.expand_dims(emb, 1)\n",
        "    emb_train = emb[:-num_validation_samples]\n",
        "    emb_val = emb[-num_validation_samples:]\n",
        "    # Build model\n",
        "    MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    with tf.device('/GPU:0'):\n",
        "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name=\"embedding_input\")\n",
        "\n",
        "        glove_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_glove_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)(sequence_input)\n",
        "        skipgram_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_skipgram_gensim_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)(sequence_input)\n",
        "        #fastText emmbedding layer:\n",
        "        #pre-Trained model: get_fastText_embedding_matrix\n",
        "        #only trained on Tweets.csv: get_fastText_gensim_embedding_matrix \n",
        "        fastText_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_fastText_gensim_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)\n",
        "        #for sentence classification: \n",
        "        #FastText: get_fastText_sentence_embedding_matrix \n",
        "        #Skip-gram: get_skipgram_sentence_embedding_matrix\n",
        "        sentence_embedding_layer = Embedding(num_sentences,\n",
        "                                    dim,\n",
        "                                    weights=[get_fastText_sentence_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)\n",
        "        \n",
        "        #Training with 2 embedding layers at a time\n",
        "        if model_size ==\"2D\":\n",
        "\n",
        "            #sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "            embedded_sequences = fastText_embedding_layer(sequence_input)\n",
        "            #FastText: embedded_sequences\n",
        "            #Glove: glove_embedding_layer\n",
        "            #Skipgram: skipgram_embedding_layer\n",
        "            combined=  tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([embedded_sequences, glove_embedding_layer])\n",
        "            x = Conv2D(128, 5, activation='relu')(combined)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "\n",
        "        #Model for sentence classification training --> bad\n",
        "        #kernel size must be 1 and pool_size as well, cause given array is one dimensional     \n",
        "        if model_size ==\"sentence\":\n",
        "\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = sentence_embedding_layer(sequence_input)\n",
        "\n",
        "            x = Conv1D(64, 1, padding='valid', activation='relu', strides=1) (embedded_sequences)\n",
        "            x = MaxPooling1D(pool_size=1) (x)\n",
        "            x = LSTM(70) (x)\n",
        "            x = Dense(35) (x)\n",
        "            x = Dense(1) (x)\n",
        "            x = Activation('sigmoid') (x)\n",
        "\n",
        "            preds = Dense(3, activation='softmax')(x)\n",
        "            model = Model(inputs=sequence_input, outputs=preds)\n",
        "\n",
        "            model.compile(loss='binary_crossentropy',\n",
        "                        optimizer='adam',\n",
        "                        metrics=['accuracy'])\n",
        "            \n",
        "            model.summary()\n",
        "            \n",
        "            model.fit(w_train, v_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(w_val, v_val))\n",
        "            return model\n",
        "\n",
        "        #1DConv Model\n",
        "        #training with one embedding layer at a time\n",
        "        #fastText_embedding_layer:\n",
        "        #Glove_embedding_layer:\n",
        "        #skipgram_embedding_layer:\n",
        "        if model_size ==\"1D\":\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = fastText_embedding_layer(sequence_input)\n",
        "\n",
        "            x = Conv1D(64, 5, padding='valid', activation='relu', strides=1) (embedded_sequences)\n",
        "            x = MaxPooling1D(pool_size=4) (x)\n",
        "            x = LSTM(70) (x)\n",
        "            x = Dense(35) (x)\n",
        "            x = Dense(1) (x)\n",
        "            x = Activation('sigmoid') (x)\n",
        "\n",
        "        #not used, would be for 3 embeddings at a time\n",
        "        if model_size ==\"large\":\n",
        "            skipgram_sentence_embedding = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    #embeddings_initializer=Constant(get_skipgram_gensim_embedding_matrix(text, epochs=1)),\n",
        "                                    #weights=get_skipgram_gensim_embedding_matrix(text, epochs=1),\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)(sequence_input)\n",
        "\n",
        "            combined = tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([skipgram_embedding_layer, glove_embedding_layer,skipgram_sentence_embedding])\n",
        "            x = DenseNet121(include_top=False, weights=None, input_shape = (MAX_SEQUENCE_LENGTH, dim, 3))(combined)\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        #compiling Model\n",
        "        preds = Dense(3, activation='softmax')(x)\n",
        "        model = Model(inputs=sequence_input, outputs=preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='rmsprop',\n",
        "                    metrics=['acc'])\n",
        "\n",
        "        model.summary()\n",
        "        # plot_model(model, to_file='model_combined.png')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_val, y_val))\n",
        "        \n",
        "        # scores = model.evaluate(x_val, y_val)\n",
        "        # print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "        if save:\n",
        "          # data/\n",
        "            model.save(\"sentqs_full.h5\")\n",
        "        return model\n",
        "\n",
        "def tsne_embedding(X):\n",
        "    print(\"Starting TSNE\\n\")\n",
        "    for p in [5,25,50,75,100]:\n",
        "        tsne = TSNE(n_components=2, init='random',\n",
        "             random_state=0, perplexity=p)\n",
        "        xl = tsne.fit_transform(X)\n",
        "        # data/\n",
        "        np.save(\"sentqs_tsne_\"+str(p)+\".npy\",xl)\n",
        "        print(\"Finished TSNE\\n\")\n",
        "\n",
        "\n",
        "def describe_dataset(tweets,labels):\n",
        "    data = pd.DataFrame([tweets, labels]).T\n",
        "    description = data.describe()\n",
        "    print(description)\n",
        "\n",
        "    print(\"Class Counts:\")\n",
        "    class_counts = data.groupby(1).size()\n",
        "\n",
        "    x = class_counts.to_numpy()\n",
        "    keys = class_counts.keys().to_list()\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.bar(keys, x)\n",
        "    plt.ylabel(\"Tweet Count\")\n",
        "    plt.xticks(range(len(keys)), keys, rotation=45)\n",
        "    plt.xlabel(\"Hashtags\")\n",
        "    plt.tight_layout()\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_class_dist.pdf\", dpi=1000, transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_eigenspectrum(x):\n",
        "    values = np.linalg.svd(x,compute_uv=False)\n",
        "    plt.bar(range(101), values[:101], align='center')\n",
        "    plt.ylabel(\"Eigenvalue\")\n",
        "    # plt.tight_layout()\n",
        "    plt.xlabel(\"No.\")\n",
        "    plt.xticks([0, 20, 40, 60, 80, 100], [1, 20, 40, 60, 80, 100])\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_spectra.pdf\", transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "# Y for fastText\n",
        "def plot_tsne(X:None,labels):\n",
        "    tsne_embedding(X)\n",
        "\n",
        "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "    for p in [5, 25, 50, 75, 100]:\n",
        "      # data/\n",
        "        d = np.load(\"sentqs_tsne_\" + str(p) + \".npy\")\n",
        "        for idx, l in enumerate(list(set(labels))):\n",
        "            c = np.where(y == idx)[0]\n",
        "            x = d[c, :]\n",
        "            plt.scatter(x[:, 0], x[:, 1], s=.5, label=l)\n",
        "            plt.legend(markerscale=10., bbox_to_anchor=(1, 1.02))\n",
        "        plt.ylabel(\"$x_1$\")\n",
        "        plt.xlabel(\"$x_2$\")\n",
        "        plt.tight_layout()\n",
        "        # plots/\n",
        "        plt.savefig('sentqs_tsne_plot_' + str(p) + \".pdf\", dpi=1000, transparent=True)\n",
        "        plt.show()\n",
        "\n",
        "def create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment):\n",
        "\n",
        "    Xs = X[source_idx]\n",
        "    Xt = X[target_idx]\n",
        "    Ys = sentiment[source_idx]\n",
        "    Yt = sentiment[target_idx]\n",
        "    data = [Xs,Ys,Xt,Yt]\n",
        "    # data/\n",
        "    np.savez('sentqs_dataset.npz', *data)\n",
        "    # data/\n",
        "    sio.savemat('sentqs_dataset.mat', {'Xs': Xs, 'Xt': Xt, 'Ys': Ys, 'Yt': Yt})\n",
        "    source_tweets = [tweets[i] for i in source_idx]\n",
        "    target_tweets = [tweets[i] for i in target_idx]\n",
        "\n",
        "    # data/\n",
        "    pd.DataFrame(source_tweets).to_csv(\"sentqs_source_tweets.csv\")\n",
        "    # data/\n",
        "    pd.DataFrame(target_tweets).to_csv(\"sentqs_target_tweets.csv\")\n",
        "    return  Xs,Ys,Xt,Yt\n",
        "\n",
        "def load_sentqs_tweets():\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_preprocessed.npz\"):\n",
        "      # data/\n",
        "        loaded_data = np.load(\"sentqs_preprocessed.npz\")\n",
        "        return loaded_data['cleaned_tweets'],loaded_data[\"tweets\"], loaded_data['y'],loaded_data['sentiment'],loaded_data[\"source_idx\"],loaded_data[\"target_idx\"]\n",
        "    else:\n",
        "        hashtags = ['ADBE', 'GOOGL', 'AMZN', 'AAPL', 'ADSK', 'BKNG', 'EXPE', 'INTC', 'MSFT', 'NFLX', 'NVDA', 'PYPL', 'SBUX',\n",
        "         'TSLA', 'XEL', 'positive', 'bad', 'sad']\n",
        "\n",
        "        # Loading and preprocessing of tweets\n",
        "        # only using a small part of available Tweets to not overload the ram\n",
        "        df = pd.read_csv(\"/content/drive/My Drive/googleColabFiles/Tweets_onefourth.csv\")\n",
        "        sentiment = pd.to_numeric(df.iloc[:, -1], errors=\"raise\", downcast=\"float\")\n",
        "        labels,tweets,sentiment = seperate_tweets(df.iloc[:, 1],hashtags,sentiment)\n",
        "        cleaned_tweets = clean_text(tweets)\n",
        "\n",
        "        y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "\n",
        "        source_idx,target_idx = create_domain_adaptation_index(tweets,labels,sentiment)\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_preprocessed.npz\",tweets=tweets, cleaned_tweets=cleaned_tweets, y=y,sentiment=sentiment,source_idx=source_idx,target_idx=target_idx)\n",
        "        return cleaned_tweets,tweets, y,sentiment,source_idx,target_idx\n",
        "\n",
        "def create_domain_adaptation_index(tweets,labels,sentiment):\n",
        "    labels = np.array([s if \"#bad\" not in s else \"#sad\" for s in labels])\n",
        "    source_idx  = np.array([i for i,val in enumerate(labels) if val== \"#sad\" or val == \"#bad\" ])\n",
        "    target_idx = np.array([i for i,val in enumerate(labels) if val != \"#sad\" and val != \"#bad\" ])\n",
        "    return source_idx,target_idx\n",
        "\n",
        "\n",
        "def main_preprocessing(mode=\"multi_semantic_embedding\"):\n",
        "\n",
        "    # Load neccessary informations about the dataset\n",
        "    cleaned_tweets,tweets,hashtags,sentiment, source_idx, target_idx = load_sentqs_tweets()\n",
        "\n",
        "    if mode == \"multi_semantic_embedding\":\n",
        "\n",
        "        # Obtain embeddings and train deep learning model\n",
        "        model = generate_embedding_model(cleaned_tweets,sentiment,source_idx,target_idx,model_size=\"2D\")\n",
        "        # supervised training and testing with fastText on Tweets.csv:\n",
        "        # text_classification_ft(cleaned_tweets, sentiment)\n",
        "\n",
        "    elif mode == \"train_embedding\":\n",
        "        #Obtain skipgram embedding only\n",
        "        #Create feature representation: TFIDF-Variants and skipgram embedding with 1000 dimension and negative sampling\n",
        "        # Output will be saved to disk\n",
        "        get_glove_embedding_matrix(cleaned_tweets)\n",
        "        get_skipgram_gensim_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "        # Sentence Skipgram is the base feature representation of the datatset\n",
        "        X = get_skipgram_sentence_embedding_matrix(cleaned_tweets)\n",
        "        create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment)\n",
        "        # Another possible embedding:\n",
        "        # FastText sentence embedding:\n",
        "        get_fastText_sentence_embedding_matrix(cleaned_tweets)\n",
        "        # only trained on tweets.csv:\n",
        "        Y = get_fastText_gensim_embedding(cleaned_tweets)\n",
        "        # pretrained\n",
        "        Z = get_fastText_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif mode == \"describe_dataset\":\n",
        "        # # Describe dataset with some common characteristics\n",
        "        describe_dataset(cleaned_tweets,hashtags)\n",
        "\n",
        "        # ## Plot eigenspectrum of embeddings\n",
        "        #data/\n",
        "        X = np.load(\"sentqs_skipgram_sentence_embedding.npz\",allow_pickle=True)\n",
        "        print(X.files)\n",
        "        \n",
        "        plot_eigenspectrum(X['embedding'])\n",
        "\n",
        "        # ## Plot representation of 2 dimensional tsne embedding\n",
        "        plot_tsne(X['embedding'],sentiment)\n",
        "\n",
        "    else:\n",
        "        ## Loads the data into the program and trains machine learning model\n",
        "        load_data_run_classification()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Obtain the all files of the dataset preprocessing, including plots, feature representation etc.\n",
        "    # After running this file you will find the corresponding files for classification in the data folder\n",
        "    # select mode:\n",
        "    main_preprocessing(\"multi_semantic_embedding\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 31215 unique tokens.\n",
            "Shape of data tensor: (15382, 335)\n",
            "Shape of label tensor: (15382, 3)\n",
            "indices:  (15382,)\n",
            "data:  (15382, 335)\n",
            "Loaded Skipgram embedding.\n",
            "Loaded Glove embedding.\n",
            "Loaded Skipgram embedding.\n",
            "Loaded fastText_gensim embedding.\n",
            "Loaded fastText_sentence embedding.\n",
            "Model: \"model_23\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "embedding_input (InputLayer)    [(None, 331)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_106 (Embedding)       (None, 331, 200)     6243200     embedding_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_104 (Embedding)       (None, 331, 200)     6243200     embedding_input[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 331, 200, 2)  0           embedding_106[0][0]              \n",
            "                                                                 embedding_104[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 327, 196, 128 6528        lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 65, 39, 128)  0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 61, 35, 128)  409728      max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 12, 7, 128)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 3, 128)    409728      max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_3 (GlobalM (None, 128)          0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_69 (Dense)                (None, 128)          16512       global_max_pooling2d_3[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_70 (Dense)                (None, 3)            387         dense_69[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 13,329,283\n",
            "Trainable params: 842,883\n",
            "Non-trainable params: 12,486,400\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 331) for input Tensor(\"embedding_input_26:0\", shape=(None, 331), dtype=int32), but it was called on an input with incompatible shape (None, 335).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 331) for input Tensor(\"embedding_input_26:0\", shape=(None, 331), dtype=int32), but it was called on an input with incompatible shape (None, 335).\n",
            " 17/128 [==>...........................] - ETA: 22s - loss: 1.0931 - acc: 0.4945"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-eb9c698b87a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;31m# After running this file you will find the corresponding files for classification in the data folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0;31m# select mode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0mmain_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_semantic_embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-eb9c698b87a4>\u001b[0m in \u001b[0;36mmain_preprocessing\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;31m# Obtain embeddings and train deep learning model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_embedding_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_tweets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0;31m# supervised training and testing with fastText on Tweets.csv:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;31m# text_classification_ft(cleaned_tweets, sentiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-eb9c698b87a4>\u001b[0m in \u001b[0;36mgenerate_embedding_model\u001b[0;34m(text, y, source_idx, target_idx, batch_size, epochs, save, dim, val_split, model_size)\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m                 validation_data=(x_val, y_val))\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;31m# scores = model.evaluate(x_val, y_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOOi63mz4pI",
        "colab_type": "text"
      },
      "source": [
        "sentqs_demo.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3515sJZ-z8RC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "07b27389-69eb-4144-cf1b-b860e8bd9bfd"
      },
      "source": [
        "# Error: default_graph when using only keras and not tensorflow.keras\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Conv2D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import requests\n",
        "import sys\n",
        "import numpy as np\n",
        "# from NBT import NBT --> already in notebook\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import preprocessing\n",
        "link = \"https://cloud.fhws.de/index.php/s/M4rkbHj9FfW6YKo/download\"\n",
        "# data/\n",
        "file_name = \"/content/drive/My Drive/googleColabFiles/sentqs_dataset.npz\"\n",
        "\n",
        "def download_data():\n",
        "    with open(file_name, \"wb\") as f:\n",
        "            print(\"Downloading %s\" % file_name)\n",
        "            response = requests.get(link, stream=True)\n",
        "            total_length = response.headers.get('content-length')\n",
        "\n",
        "            if total_length is None: # no content length header\n",
        "                f.write(response.content)\n",
        "            else:\n",
        "                dl = 0\n",
        "                total_length = int(total_length)\n",
        "                for data in response.iter_content(chunk_size=4096):\n",
        "                    dl += len(data)\n",
        "                    f.write(data)\n",
        "                    done = int(50 * dl / total_length)\n",
        "                    sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "    data = np.load(file_name,allow_pickle=True)\n",
        "    Xs = data[\"arr_0\"]\n",
        "    Ys = data[\"arr_1\"]\n",
        "    Xt = data[\"arr_2\"]\n",
        "    Yt = data[\"arr_3\"]\n",
        "    return Xs,Ys,Xt,Yt\n",
        "\n",
        "#Xs,Ys,Xt,Yt = download_data()\n",
        "\n",
        "#If dataset file is already downloaded\n",
        "data = np.load(file_name,allow_pickle=True)\n",
        "Xs = data[\"arr_0\"]\n",
        "Ys = data[\"arr_1\"]\n",
        "Xt = data[\"arr_2\"]\n",
        "Yt = data[\"arr_3\"]\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "# Training\n",
        "batch_size = 256\n",
        "epochs = 2\n",
        "# LSTM\n",
        "\n",
        "lstm_output_size = 70\n",
        "Xs = np.expand_dims(Xs, 2)\n",
        "Xt = np.expand_dims(Xt, 2)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "model.add(LSTM(lstm_output_size))\n",
        "model.add(Dense(35))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# model.summary()\n",
        "print('Train...')\n",
        "model.fit(Xs, Ys,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(Xt, Yt))\n",
        "score, acc = model.evaluate(Xt, Yt, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Epoch 1/2\n",
            "84/84 [==============================] - 3s 34ms/step - loss: -26.1977 - accuracy: 0.1861 - val_loss: 14.7792 - val_accuracy: 0.6096\n",
            "Epoch 2/2\n",
            "84/84 [==============================] - 2s 27ms/step - loss: -117.8802 - accuracy: 0.1861 - val_loss: 42.2398 - val_accuracy: 0.6096\n",
            "157/157 [==============================] - 1s 7ms/step - loss: 42.2398 - accuracy: 0.6096\n",
            "Test score: 42.23982620239258\n",
            "Test accuracy: 0.6096078157424927\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}