{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "10TrkrLAej16spZ6zrIGyVV3eEKAZ95hK",
      "authorship_tag": "ABX9TyPdVhwh2pzyfFQIzLgclVJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuellerLeonard/Skip-Gram_fastText/blob/master/fastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yuhFPWjyQGN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b79a5c95-cc76-4742-a708-d9e14253143e"
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install keras\n",
        "!pip install requests\n",
        "!pip install fasttext"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/31/bf/c28971266ca854a64f4b26f07c4112ddd61f30b4d1f18108b954a746f8ea/tensorflow_gpu-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
            "\u001b[K     |████████████████████████████████| 516.2MB 33kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.1)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.30.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (49.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.2.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (49.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3020488 sha256=aa50d38cf3fa4447ea0ca627b85620b6fd229109dd70576d6cf9697acf11ff46\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj3sZzNZyqzG",
        "colab_type": "text"
      },
      "source": [
        "Needed classes for imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXRHeo3Cy7XR",
        "colab_type": "text"
      },
      "source": [
        "cleanup.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0hVox5sytjw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3878f9f4-11bb-4cf5-92fc-a5ff01e95a74"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(tweets: List[str], lang: str = 'english') -> List[str]:\n",
        "    \"\"\"\n",
        "    performes stemming and other \"cleanup\"\n",
        "    :param tweets:\n",
        "    :param lang:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    data: List[str] = tweets\n",
        "    cleaned_text = []\n",
        "    stops = set(stopwords.words(lang))\n",
        "\n",
        "    # no ! and .\n",
        "    table = str.maketrans(dict.fromkeys(\"\"\"\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\"\"\"))\n",
        "\n",
        "    for text in data:\n",
        "        text = re.sub(r'https?://[^\\s]*', ' tokenlink ', text)\n",
        "        text = re.sub(r'\\.\\.\\.', ' tokendotdotdot ', text)\n",
        "        text = re.sub(r'xD', ' tokenxd ', text)\n",
        "        text = re.sub(r':\\)', ' tokenxbrackethappy ', text)\n",
        "        text = re.sub(r':\\(', ' tokenxbracketsad ', text)\n",
        "        text = re.sub(r':-\\)', ' tokennosehappy ', text)\n",
        "        text = re.sub(r':-\\(', ' tokennosesad ', text)\n",
        "        text = re.sub(r':D', ' tokenxcheer ', text)\n",
        "        text = re.sub(r':-S', ' tokens ', text)\n",
        "\n",
        "\n",
        "        ## Convert words to lower case and split them\n",
        "        text = text.lower().split()\n",
        "\n",
        "        text = \" \".join(text)\n",
        "\n",
        "        # Clean the text\n",
        "        text = re.sub(r\"(?<!\\.)\\.(?!\\.)\", \" \", text)\n",
        "        text = re.sub(r\"[^A-Za-z0-9^,!./'+-=]\", \" \", text)\n",
        "        text = re.sub(r\"what's\", \"what is \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"i'm\", \"i am \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "        text = re.sub(r\",\", \" \", text)\n",
        "        text = re.sub(r\"\\.\", \" \", text)\n",
        "        text = re.sub(r\"!\", \" ! \", text)\n",
        "        text = re.sub(r\"/\", \" \", text)\n",
        "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "        text = re.sub(r\"\\+\", \" + \", text)\n",
        "        text = re.sub(r\"-\", \" - \", text)\n",
        "        text = re.sub(r\"=\", \" = \", text)\n",
        "        text = re.sub(r\"'\", \" \", text)\n",
        "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "        text = re.sub(r\":\", \" : \", text)\n",
        "        text = re.sub(r\" e g \", \" eg \", text)\n",
        "        text = re.sub(r\" b g \", \" bg \", text)\n",
        "        text = re.sub(r\" u s \", \" american \", text)\n",
        "        text = re.sub(r\"\\0s\", \"0\", text)\n",
        "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "        text = re.sub(r\"e - mail\", \"email\", text)\n",
        "        text = re.sub(r\"j k\", \"jk\", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "        ## Remove puncuation\n",
        "        text = text.translate(table)\n",
        "\n",
        "        text = text.split()\n",
        "\n",
        "        ## Remove stop words[^\\s][^\\s]\n",
        "        #text = [w for w in text if not w in stops]\n",
        "\n",
        "        stemmer = SnowballStemmer(lang)\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "        cleaned_text.append(text)\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-0e-uMFjt2p",
        "colab_type": "text"
      },
      "source": [
        "Return the cleaned Text to see if it is useable for fastText (append __Label__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93jtne_AlLzN",
        "colab_type": "text"
      },
      "source": [
        "use fastText classifier to vectorize the Tweets.csv file --> Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb6f1fLjn8zk",
        "colab_type": "text"
      },
      "source": [
        "Create another notebook with other Datasets to train the classifiers on (Skip-gram and fastText)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsPd_IYrzGMo",
        "colab_type": "text"
      },
      "source": [
        "NBT.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzwORX2szK88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d33af4b4-b6b7-4af1-8220-59c07cb1d674"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "class NBT():\n",
        "    \"\"\"\n",
        "    Nyström Basis Transfer Service Class\n",
        "    Published in:\n",
        "    Christoph Raab, Frank-Michael Schleif,\n",
        "    Transfer learning extensions for the probabilistic classification vector machine,Neurocomputing,2019,\n",
        "    https://doi.org/10.1016/j.neucom.2019.09.104.\n",
        "    Functions\n",
        "    ----------\n",
        "    nys_basis_transfer: Transfer Basis from Target to Source Domain.\n",
        "    data_augmentation: Augmentation of data by removing or upsampling of source data\n",
        "    Examples\n",
        "    --------\n",
        "    >>> #Imports\n",
        "    >>> import os\n",
        "    >>> import scipy.io as sio\n",
        "    >>> from sklearn.svm import SVC\n",
        "    >>> os.chdir(os.path.dirname(os.path.abspath(__file__)))\n",
        "    >>> os.chdir(os.path.join(\"datasets\",\"domain_adaptation\",\"features\",\"OfficeCaltech\"))\n",
        "    >>> amazon = sio.loadmat(\"amazon_SURF_L10.mat\")\n",
        "    >>> X = preprocessing.scale(np.asarray(amazon[\"fts\"]))\n",
        "    >>> Yt = np.asarray(amazon[\"labels\"])\n",
        "    >>>\n",
        "    >>> dslr = sio.loadmat(\"dslr_SURF_L10.mat\")\n",
        "    >>>\n",
        "    >>> Z = preprocessing.scale(np.asarray(dslr[\"fts\"]))\n",
        "    >>> Ys = np.asarray(dslr[\"labels\"])\n",
        "    >>>\n",
        "    >>> clf = SVC(gamma=1,C=10)\n",
        "    >>> clf.fit(Z,Ys)\n",
        "    >>> print(\"SVM: \"+str(clf.score(X,Yt)))\n",
        "    >>>\n",
        "    >>> nbt = NBT()\n",
        "    >>> Ys,Z = nbt.data_augmentation(Z,X.shape[0],Ys)\n",
        "    >>> X,Z = nbt.nys_basis_transfer(X,Z,Ys.flatten(),landmarks=100)\n",
        "    >>> clf = SVC(gamma=1,C=10)\n",
        "    >>> clf.fit(Z,Ys)\n",
        "    >>> print(\"SVM + NBT: \"+str(clf.score(X,Yt)))\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,landmarks=10):\n",
        "        self.landmarks = landmarks\n",
        "        pass\n",
        "\n",
        "    def nys_basis_transfer(self,X,Z,Ys=None):\n",
        "        \"\"\"\n",
        "        Nyström Basis Transfer\n",
        "        Transfers Basis of X to Z obtained by Nyström SVD\n",
        "        Implicit dimensionality reduction\n",
        "        Applications in domain adaptation or transfer learning\n",
        "        Parameters.\n",
        "        Note target,source are order sensitiv.\n",
        "        ----------\n",
        "        X : Target Matrix, where classifier is trained on\n",
        "        Z : Source Matrix, where classifier is trained on\n",
        "        Ys: Source data label, if none, classwise sampling is not applied.\n",
        "        landmarks : Positive integer as number of landmarks\n",
        "        Returns\n",
        "        ----------\n",
        "        X : Reduced Target Matrix\n",
        "        Z : Reduced approximated Source Matrix\n",
        "        \"\"\"\n",
        "        if type(X) is not np.ndarray or type(Z) is not np.ndarray:\n",
        "            raise ValueError(\"Numpy Arrays must be given!\")\n",
        "        if type(self.landmarks ) is not int or self.landmarks  < 1:\n",
        "            raise ValueError(\"Positive integer number must given!\")\n",
        "        landmarks = np.min([X.shape[0]]+[Z.shape[0]]+[self.landmarks ])\n",
        "        max_idx = np.min(list(X.shape)+list(Z.shape))\n",
        "        idx = np.random.randint(0,max_idx-1,landmarks)\n",
        "        A = X[np.ix_(idx,idx)]\n",
        "        # B = X[0:landmarks,landmarks:]\n",
        "        F = X[landmarks:,0:landmarks]\n",
        "        #C = X[landmarks:,landmarks:]\n",
        "        U, S, H = np.linalg.svd(A, full_matrices=True)\n",
        "        S = np.diag(S)\n",
        "\n",
        "        U_k = np.concatenate([U,(F @H )@np.linalg.pinv(S)])\n",
        "        #V_k = np.concatenate([H, np.matmul(np.matmul(B.T,U),np.linalg.pinv(S))])\n",
        "        X = U_k @S\n",
        "\n",
        "        if type(Ys) is np.ndarray:\n",
        "            A = self.classwise_sampling(Z,Ys,landmarks)\n",
        "        else:\n",
        "            A = Z[np.ix_(idx,idx)]\n",
        "\n",
        "        D = np.linalg.svd(A, full_matrices=True,compute_uv=False)\n",
        "        Z = U_k @ np.diag(D)\n",
        "        return preprocessing.scale(X),preprocessing.scale(Z)\n",
        "\n",
        "    def classwise_sampling(self,X,Y,n_landmarks):\n",
        "\n",
        "        A = []\n",
        "        classes = np.unique(Y)\n",
        "        c_classes = classes.size\n",
        "        samples_per_class = int(n_landmarks / c_classes)\n",
        "        for c in classes:\n",
        "            class_data = X[np.where(c == Y)]\n",
        "\n",
        "            if samples_per_class > class_data.shape[0]:\n",
        "                A = A+list(class_data)\n",
        "            else:\n",
        "                A = A+list(class_data[np.random.randint(0,class_data.shape[0],samples_per_class)])\n",
        "\n",
        "        return np.array(A)\n",
        "\n",
        "    def basis_transfer(self,X,Z):\n",
        "        \"\"\"\n",
        "         Basis Transfer\n",
        "        Transfers Basis of X to Z obtained by Nyström SVD\n",
        "        Applications in domain adaptation or transfer learning\n",
        "        Parameters.\n",
        "        Note target,source are order sensitiv.\n",
        "        ----------\n",
        "        X : Target Matrix, where classifier is trained on\n",
        "        Z : Source Matrix, where classifier is trained on\n",
        "        Returns\n",
        "        ----------\n",
        "        Z : Transferred Source Matrix\n",
        "        \"\"\"\n",
        "        L,S,R = np.linalg.svd(X,full_matrices=False);\n",
        "        D = np.linalg.svd(Z,compute_uv=False,full_matrices=False)\n",
        "        return L @ np.diag(D) @ R\n",
        "\n",
        "    def fit_predict(self, Xs, Ys, Xt, Yt):\n",
        "        '''\n",
        "        Fit and use 1NN to classify\n",
        "        :param Xs: ns * n_feature, source feature\n",
        "        :param Ys: ns * 1, source label\n",
        "        :param Xt: nt * n_feature, target feature\n",
        "        :param Yt: nt * 1, target label\n",
        "        :return: Accuracy, predicted labels of target domain, and G\n",
        "        '''\n",
        "        clf = KNeighborsClassifier(n_neighbors=1)\n",
        "        clf.fit(Xs, Ys.ravel())\n",
        "        y_pred = clf.predict(Xt)\n",
        "        acc = np.mean(y_pred == Yt.ravel())\n",
        "        return acc, y_pred\n",
        "\n",
        "\n",
        "    def data_augmentation(self,Z,required_size,Y):\n",
        "        \"\"\"\n",
        "        Data Augmentation\n",
        "        Upsampling if Z smaller as required_size via multivariate gaussian mixture\n",
        "        Downsampling if Z greater as required_size via uniform removal\n",
        "        Note both are class-wise with goal to harmonize class counts\n",
        "        ----------\n",
        "        Z : Matrix, where classifier is trained on\n",
        "        required_size : Size to which Z is reduced or extended\n",
        "        Y : Label vector, which is reduced or extended like Z\n",
        "        Returns\n",
        "        ----------\n",
        "        X : Augmented Z\n",
        "        Z : Augmented Y\n",
        "        \"\"\"\n",
        "        if type(Z) is not np.ndarray or type(required_size) is not int or type(Y) is not np.ndarray:\n",
        "            raise ValueError(\"Numpy Arrays must be given!\")\n",
        "        if Z.shape[0] == required_size:\n",
        "            return Y,Z\n",
        "        \n",
        "        _, idx = np.unique(Y, return_index=True)\n",
        "        C = Y[np.sort(idx)].flatten().tolist()\n",
        "        size_c = len(C)\n",
        "        if Z.shape[0] < required_size:\n",
        "            print(\"Source smaller target\")\n",
        "            data = np.empty((0,Z.shape[1]))\n",
        "            label = np.empty((0,1))\n",
        "            diff = required_size - Z.shape[0]\n",
        "            sample_size = int(np.floor(diff/size_c))\n",
        "            for c in C:\n",
        "                #indexes = np.where(Y[Y==c])\n",
        "                indexes =  np.where(Y==c)\n",
        "                class_data = Z[indexes,:][0]\n",
        "                m = np.mean(class_data,0) \n",
        "                sd = np.var(class_data,0)\n",
        "                sample_size = sample_size if c !=C[-1] else sample_size+np.mod(diff,size_c)\n",
        "                augmentation_data =np.vstack([np.random.normal(m, sd, size=len(m)) for i in range(sample_size)])\n",
        "                data =np.concatenate([data,class_data,augmentation_data])\n",
        "                label = np.concatenate([label,np.ones((class_data.shape[0]+sample_size,1))*c])\n",
        "            \n",
        "        if Z.shape[0] > required_size:\n",
        "            print(\"Source greater target\")\n",
        "            data = np.empty((0,Z.shape[1]))\n",
        "            label = np.empty((0,1))\n",
        "            sample_size = int(np.floor(required_size/size_c))\n",
        "            for c in C:\n",
        "                indexes = np.where(Y[Y==c])[0]\n",
        "                class_data = Z[indexes,:]\n",
        "                if len(indexes) > sample_size:\n",
        "                    sample_size = sample_size if c !=C[-1] else np.abs(data.shape[0]-required_size)\n",
        "                    y = np.random.choice(class_data.shape[0],sample_size)\n",
        "                    class_data = class_data[y,:]\n",
        "                data =np.concatenate([data,class_data])\n",
        "                label = np.concatenate([label,np.ones((class_data.shape[0],1))*c])\n",
        "        Z = data\n",
        "        Y = label\n",
        "        return Y,Z\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    import os\n",
        "    import scipy.io as sio\n",
        "    from sklearn.svm import SVC\n",
        "\n",
        "    # Load and preprocessing of data. Note normalization to N(0,1) is necessary.\n",
        "    # not needed files were directly accessed\n",
        "    # os.chdir(\"datasets/domain_adaptation/OfficeCaltech/features/Surf\")\n",
        "    amazon = sio.loadmat(\"/content/drive/My Drive/googleColabFiles/amazon_SURF_L10.mat\")\n",
        "    X = preprocessing.scale(np.asarray(amazon[\"fts\"]))\n",
        "    Yt = np.asarray(amazon[\"labels\"])\n",
        "\n",
        "    dslr = sio.loadmat(\"/content/drive/My Drive/googleColabFiles/dslr_SURF_L10.mat\")\n",
        "\n",
        "    Z = preprocessing.scale(np.asarray(dslr[\"fts\"]))\n",
        "    Ys = np.asarray(dslr[\"labels\"])\n",
        "\n",
        "    # Applying SVM without transfer learning. Accuracy should be about 10%\n",
        "    clf = SVC(gamma=1,C=10)\n",
        "    clf.fit(Z,Ys)\n",
        "    print(\"SVM without transfer \"+str(clf.score(X,Yt)))\n",
        "\n",
        "    # Beginning of NBT. Accuracy of SVM + NBT should be about 90%\n",
        "    nbt = NBT(landmarks=100)\n",
        "    # Data augmentation is necessary if Z and X have different shapes.\n",
        "    Ys,Z = nbt.data_augmentation(Z,X.shape[0],Ys)\n",
        "    X,Z = nbt.nys_basis_transfer(X,Z,Ys.flatten())\n",
        "    clf = SVC(gamma=1,C=10)\n",
        "    clf.fit(Z,Ys)\n",
        "    print(\"SVM + NBT: \"+str(clf.score(X,Yt)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM without transfer 0.10438413361169102\n",
            "Source smaller target\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "SVM + NBT: 0.9311064718162839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pC8Inwh1Uqer",
        "colab_type": "text"
      },
      "source": [
        "Download Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIzHvxVb-7RK",
        "colab_type": "text"
      },
      "source": [
        "Tweets.csv Dataset to big |\n",
        "Test if sentqs_dataset.npz works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW9yFrNaZ6-C",
        "colab_type": "text"
      },
      "source": [
        "Only for testing: Glove.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWHOlS5JzbZl",
        "colab_type": "text"
      },
      "source": [
        "sentqs_preprocess.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvlJYqlHzeJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "b47ba4b7-d1b6-4e82-b645-91ee181c8d00"
      },
      "source": [
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "# import cleanup --> already in notebook\n",
        "import keras\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, pad_sequences\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn import decomposition\n",
        "#from keras_preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import make_sampling_table\n",
        "from numpy import asarray, zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling3D, GlobalMaxPooling2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, Conv2D, MaxPooling3D, Embedding, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from scipy import spatial\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "# fastText\n",
        "#import fasttext.util\n",
        "from fasttext import load_model\n",
        "\n",
        "\n",
        "def load_data_run_classification():\n",
        "  # 'data/....npz'\n",
        "    data = np.load('/content/drive/My Drive/googleColabFiles/sentqs_dataset.npz')\n",
        "    Xs = data[\"arr_0\"]\n",
        "    Ys = data[\"arr_1\"]\n",
        "    Xt = data[\"arr_2\"]\n",
        "    Yt = data[\"arr_3\"]\n",
        "    print(\"Classification Task Test \\n\")\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Xs, Ys)\n",
        "    print(clf.score(Xt, Yt))\n",
        "\n",
        "def create_tfidf(sen,min_df=10,max_df=100):\n",
        "    print(\"Create TF-IDF\\n\")\n",
        "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
        "    X = vectorizer.fit_transform(sen)\n",
        "    return X.toarray()\n",
        "\n",
        "def save_dataset(X, y, prefix =\"\"):\n",
        "    print(\"Save Dataset \\n\")\n",
        "    y = np.array(y)[:, None]\n",
        "    dataset = np.concatenate([y,X], axis=1)\n",
        "\n",
        "    # data/\n",
        "    np.save(\"sentqs_da_\"+str(prefix)+\".npy\",dataset)\n",
        "    return dataset\n",
        "\n",
        "def seperate_tweets(data,hashtags,sentiment):\n",
        "    print(\"Seperate Tweets \\n\")\n",
        "    labels = []\n",
        "    tweets = []\n",
        "    sentiment_new = []\n",
        "    for t,s in zip(data,sentiment):\n",
        "        for h in hashtags:\n",
        "            t = t.lower()\n",
        "            h = \"#\" + h.lower()\n",
        "            if h in t:\n",
        "                labels.append(h)\n",
        "                tweets.append(t.replace(h,\" \"))\n",
        "                sentiment_new.append(s)\n",
        "                break\n",
        "\n",
        "    return labels,tweets,sentiment_new\n",
        "\n",
        "def generate_data(corpus, window_size, V):\n",
        "    for words in corpus:\n",
        "        couples, labels = skipgrams(words, V, window_size, negative_samples=1, shuffle=True,sampling_table=make_sampling_table(V, sampling_factor=1e-05))\n",
        "        if couples:\n",
        "            X, y = zip(*couples)\n",
        "            X = np_utils.to_categorical(X, V)\n",
        "            y = np_utils.to_categorical(y, V)\n",
        "            yield X, y\n",
        "\n",
        "def get_glove_embedding_matrix(texts, dim=200):\n",
        "    # data/\n",
        "    if os.path.isfile(\"sentqs_glove_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_glove_embedding.npz\")\n",
        "        print('Loaded Glove embedding.')\n",
        "        return loaded_embedding['embedding']\n",
        "    else:\n",
        "        # first, build index mapping words in the embeddings set\n",
        "        # to their embedding vector\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "        word_index = tokenizer.word_index\n",
        "\n",
        "        print('Indexing word vectors.')\n",
        "\n",
        "        embeddings_index = {}\n",
        "        # data/\n",
        "        # evtl. größeren Datensatz wenn möglich verwenden\n",
        "        with open('/content/drive/My Drive/googleColabFiles/dataset/glove.twitter.27B.200d.txt', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "                embeddings_index[word] = coefs\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "        print('Preparing embedding matrix.')\n",
        "\n",
        "        # prepare embedding matrix\n",
        "        num_words = len(word_index) + 1\n",
        "        embedding_matrix = np.zeros((num_words, dim))\n",
        "        counter = 0\n",
        "        for word, i in word_index.items():\n",
        "            # if i >= MAX_NUM_WORDS:\n",
        "            #    counter +=1\n",
        "            #    continue\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_glove_embedding.npz\", embedding=embedding_matrix)\n",
        "        return embedding_matrix\n",
        "\n",
        "def get_fastText_embedding_matrix(texts, BASE_DIR = '', MAX_SEQUENCE_LENGTH = 1000, EMBEDDING_DIM = 200):\n",
        "# dataset dimension must be 200\n",
        "    FT_MODEL = \"/content/drive/My Drive/googleColabFiles/dataset/cc.en200.bin\"\n",
        "    #print(\"step 1\")\n",
        "    #FT_MODEL =fasttext.load_model('/content/drive/My Drive/googleColabFiles/dataset/cc.en.300.bin')\n",
        "    #print(\"loading model\")\n",
        "    #fasttext.util.reduce_model(FT_MODEL, EMBEDDING_DIM)\n",
        "    #print(\"dimensions reduced\")\n",
        "\n",
        "    #FT_MODEL.get_dimension()\n",
        "\n",
        "    if os.path.isfile(\"sentqs_fastText_embedding.npz\"):\n",
        "      saved_embedding = np.load(\"sentqs_fastText_embedding.npz\")\n",
        "      print('loaded fastText embedding.')\n",
        "      return saved_embedding['embedding']\n",
        "\n",
        "    else:\n",
        "      #pretrained vectors:\n",
        "      print('Indexing word vectors.')\n",
        "\n",
        "      filename, file_extension = os.path.splitext(FT_MODEL)\n",
        "\n",
        "      if file_extension == '.vec':\n",
        "          embeddings_index = {}\n",
        "          with open(os.path.join(FT_MODEL)) as f:\n",
        "              for line in f:\n",
        "                  values = line.split()\n",
        "                  word = values[0]\n",
        "                  coefs = np.asarray(values[1:], dtype='float32')\n",
        "                  embeddings_index[word] = coefs\n",
        "            \n",
        "          print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "      # use pretrained model on tweets:\n",
        "      print('Found %s texts.' % len(texts))\n",
        "      # finally, vectorize the text samples into a 2D integer tensor\n",
        "      # num_words = size of vocab\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(texts)\n",
        "      sequences = tokenizer.texts_to_sequences(texts)\n",
        "      word_index = tokenizer.word_index\n",
        "      print('Found %s unique tokens.' % len(word_index))\n",
        "      # look this up\n",
        "      # data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "      # labels = to_categorical(np.asarray(labels))\n",
        "      # print('Shape of data tensor:', data.shape)\n",
        "      # print('Shape of label tensor:', labels.shape)\n",
        "      # labels = np.asarray(labels)\n",
        "\n",
        "      print('Preparing embedding matrix.')\n",
        "\n",
        "      # load the fasttext model\n",
        "      f = load_model(FT_MODEL)\n",
        "\n",
        "      # prepare embedding matrix\n",
        "      # num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
        "      # num_words\n",
        "      num_words = len(word_index) + 1\n",
        "      emb_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "      for word, i in word_index.items():\n",
        "        # if i >= MAX_NUM_WORDS:\n",
        "          # continue\n",
        "        # embedding_vector = embeddings_index.get(word)\n",
        "          embedding_vector = f.get_word_vector(word)\n",
        "          if embedding_vector is not None:\n",
        "              # words not found in embedding index will be all-zeros.\n",
        "              emb_matrix[i] = embedding_vector\n",
        "\n",
        "      np.savez_compressed(\"sentqs_fastText_embedding.npz\", embedding = emb_matrix)\n",
        "      return emb_matrix\n",
        "\n",
        "# epochs 100 to 10\n",
        "# batch_size, epochs je nach Menge an Daten die eingespeißt werden\n",
        "def get_skipgram_sentence_embedding_matrix(text, dim=200, batch_size=64, window_size=5, epochs = 1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_sentence_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_sentence_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        text = [''.join(x) for x in text]\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "        corpus = t.texts_to_sequences(text)\n",
        "        print(corpus)\n",
        "        V = len(t.word_index)\n",
        "        step_size = len(corpus) // batch_size\n",
        "        model = Sequential()\n",
        "        model.add(Dense(dim, input_dim=V, activation=\"softmax\"))\n",
        "        model.add(Dense(V, input_dim=dim, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "        model.summary()\n",
        "\n",
        "        model.fit(generate_data(corpus, window_size, V), epochs=epochs, steps_per_epoch=step_size)\n",
        "        # model.save(\"data/sentqs_full_skigram_arc.h5\")\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        print(\"step 1\")\n",
        "        enc = mlb.fit_transform(corpus)\n",
        "        print(\"step 2\")\n",
        "        emb = enc @ model.get_weights()[0]\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_sentence_embedding\", embedding=emb)\n",
        "        return emb\n",
        "\n",
        "def  get_skipgram_gensim_embedding_matrix(text, dim = 200, window_size=5, min_word_occurance=1, epochs=1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_gensim_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_gensim_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        x = [row.split(' ') for row in text]\n",
        "        model = Word2Vec(x, size=dim, window=window_size, min_count=min_word_occurance, workers=4, sg=1) #sg = 1: use skipgram\n",
        "\n",
        "        words = model.wv.vocab.keys()\n",
        "        vocab_size = len(words)\n",
        "        print(\"Vocab size\", vocab_size)\n",
        "\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "\n",
        "        # total vocabulary size plus 0 for unknown words\n",
        "        # vocab_size = len(vocab) + 1\n",
        "        # define weight matrix dimensions with all 0\n",
        "        weight_matrix = zeros((vocab_size, dim))\n",
        "        # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "        for word, i in t.word_index.items():\n",
        "            if i > vocab_size: break\n",
        "            if word in model.wv.vocab.keys():\n",
        "                weight_matrix[i] = model.wv[word]\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_gensim_embedding\", embedding=weight_matrix)\n",
        "        return weight_matrix\n",
        "\n",
        "def generate_embedding_model(text, y,source_idx,target_idx,batch_size=32, epochs = 30, save = True, dim = 200, val_split=0.2,model_size=\"ftTest\"):\n",
        "    # Preprocessing\n",
        "    #MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    MAX_SEQUENCE_LENGTH = 335\n",
        "    texts = [''.join(x) for x in text]\n",
        "    # finally, vectorize the text samples into a 2D integer tensor\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    labels = to_categorical(np.asarray(y))\n",
        "    print('Shape of data tensor:', data.shape)\n",
        "    print('Shape of label tensor:', labels.shape)\n",
        "    num_words = len(word_index) + 1\n",
        "\n",
        "    # split the data into a training set and a validation set\n",
        "    indices = np.arange(data.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    data = data[indices]\n",
        "    labels = labels[indices]\n",
        "    num_validation_samples = int(val_split * data.shape[0])\n",
        "\n",
        "    x_train = data[source_idx]\n",
        "    y_train = labels[source_idx]\n",
        "    x_val = data[target_idx]\n",
        "    y_val = labels[target_idx]\n",
        "\n",
        "    emb = get_skipgram_gensim_embedding_matrix(text, epochs=1)\n",
        "    emb = np.expand_dims(emb, 1)\n",
        "    emb_train = emb[:-num_validation_samples]\n",
        "    emb_val = emb[-num_validation_samples:]\n",
        "    # Build model\n",
        "    MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    with tf.device('/GPU:0'):\n",
        "        sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name=\"embedding_input\")\n",
        "\n",
        "        glove_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_glove_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)(sequence_input)\n",
        "        skipgram_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_skipgram_gensim_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)(sequence_input)\n",
        "        fastText_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_fastText_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)\n",
        "\n",
        "        if model_size ==\"medium\":\n",
        "\n",
        "            combined=  tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([skipgram_embedding_layer, glove_embedding_layer])\n",
        "            x = Conv2D(128, 5, activation='relu')(combined)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "        \n",
        "        if model_size ==\"ftTest\":\n",
        "\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = fastText_embedding_layer(sequence_input)\n",
        "            single=  tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([embedded_sequences])\n",
        "            x = Conv2D(128, 5, activation='relu')(single)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "\n",
        "\n",
        "\n",
        "        if model_size ==\"large\":\n",
        "            skipgram_sentence_embedding = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    #embeddings_initializer=Constant(get_skipgram_gensim_embedding_matrix(text, epochs=1)),\n",
        "                                    #weights=get_skipgram_gensim_embedding_matrix(text, epochs=1),\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)(sequence_input)\n",
        "\n",
        "            combined = tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([skipgram_embedding_layer, glove_embedding_layer,skipgram_sentence_embedding])\n",
        "            x = DenseNet121(include_top=False, weights=None, input_shape = (MAX_SEQUENCE_LENGTH, dim, 3))(combined)\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        preds = Dense(3, activation='softmax')(x)\n",
        "        model = Model(inputs=sequence_input, outputs=preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='rmsprop',\n",
        "                    metrics=['acc'])\n",
        "\n",
        "        model.summary()\n",
        "        # plot_model(model, to_file='model_combined.png')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_val, y_val))\n",
        "        \n",
        "        # scores = model.evaluate(x_val, y_val)\n",
        "        # print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "        if save:\n",
        "          # data/\n",
        "            model.save(\"sentqs_full.h5\")\n",
        "        return model\n",
        "\n",
        "def tsne_embedding(X):\n",
        "    print(\"Starting TSNE\\n\")\n",
        "    for p in [5,25,50,75,100]:\n",
        "        tsne = TSNE(n_components=2, init='random',\n",
        "             random_state=0, perplexity=p)\n",
        "        xl = tsne.fit_transform(X)\n",
        "        # data/\n",
        "        np.save(\"sentqs_tsne_\"+str(p)+\".npy\",xl)\n",
        "        print(\"Finished TSNE\\n\")\n",
        "\n",
        "\n",
        "def describe_dataset(tweets,labels):\n",
        "    data = pd.DataFrame([tweets, labels]).T\n",
        "    description = data.describe()\n",
        "    print(description)\n",
        "\n",
        "    print(\"Class Counts:\")\n",
        "    class_counts = data.groupby(1).size()\n",
        "\n",
        "    x = class_counts.to_numpy()\n",
        "    keys = class_counts.keys().to_list()\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.bar(keys, x)\n",
        "    plt.ylabel(\"Tweet Count\")\n",
        "    plt.xticks(range(len(keys)), keys, rotation=45)\n",
        "    plt.xlabel(\"Hastags\")\n",
        "    plt.tight_layout()\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_class_dist.pdf\", dpi=1000, transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_eigenspectrum(x):\n",
        "    values = np.linalg.svd(x,compute_uv=False)\n",
        "    plt.bar(range(101), values[:101], align='center')\n",
        "    plt.ylabel(\"Eigenvalue\")\n",
        "    # plt.tight_layout()\n",
        "    plt.xlabel(\"No.\")\n",
        "    plt.xticks([0, 20, 40, 60, 80, 100], [1, 20, 40, 60, 80, 100])\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_spectra.pdf\", transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "# Y for fastText\n",
        "def plot_tsne(X:None,labels):\n",
        "    tsne_embedding(X)\n",
        "\n",
        "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "    for p in [5, 25, 50, 75, 100]:\n",
        "      # data/\n",
        "        d = np.load(\"sentqs_tsne_\" + str(p) + \".npy\")\n",
        "        for idx, l in enumerate(list(set(labels))):\n",
        "            c = np.where(y == idx)[0]\n",
        "            x = d[c, :]\n",
        "            plt.scatter(x[:, 0], x[:, 1], s=.5, label=l)\n",
        "            plt.legend(markerscale=10., bbox_to_anchor=(1, 1.02))\n",
        "        plt.ylabel(\"$x_1$\")\n",
        "        plt.xlabel(\"$x_2$\")\n",
        "        plt.tight_layout()\n",
        "        # plots/\n",
        "        plt.savefig('sentqs_tsne_plot_' + str(p) + \".pdf\", dpi=1000, transparent=True)\n",
        "        plt.show()\n",
        "\n",
        "def create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment):\n",
        "\n",
        "    Xs = X[source_idx]\n",
        "    Xt = X[target_idx]\n",
        "    Ys = sentiment[source_idx]\n",
        "    Yt = sentiment[target_idx]\n",
        "    data = [Xs,Ys,Xt,Yt]\n",
        "    # data/\n",
        "    np.savez('sentqs_dataset.npz', *data)\n",
        "    # data/\n",
        "    sio.savemat('sentqs_dataset.mat', {'Xs': Xs, 'Xt': Xt, 'Ys': Ys, 'Yt': Yt})\n",
        "    source_tweets = [tweets[i] for i in source_idx]\n",
        "    target_tweets = [tweets[i] for i in target_idx]\n",
        "\n",
        "    # data/\n",
        "    pd.DataFrame(source_tweets).to_csv(\"sentqs_source_tweets.csv\")\n",
        "    # data/\n",
        "    pd.DataFrame(target_tweets).to_csv(\"sentqs_target_tweets.csv\")\n",
        "    return  Xs,Ys,Xt,Yt\n",
        "\n",
        "def load_sentqs_tweets():\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_preprocessed.npz\"):\n",
        "      # data/\n",
        "        loaded_data = np.load(\"sentqs_preprocessed.npz\")\n",
        "        return loaded_data['cleaned_tweets'],loaded_data[\"tweets\"], loaded_data['y'],loaded_data['sentiment'],loaded_data[\"source_idx\"],loaded_data[\"target_idx\"]\n",
        "    else:\n",
        "        hashtags = ['ADBE', 'GOOGL', 'AMZN', 'AAPL', 'ADSK', 'BKNG', 'EXPE', 'INTC', 'MSFT', 'NFLX', 'NVDA', 'PYPL', 'SBUX',\n",
        "         'TSLA', 'XEL', 'positive', 'bad', 'sad']\n",
        "\n",
        "        # Loading and preprocessing of tweets\n",
        "        # only using a small part of available Tweets to not overload the ram\n",
        "        df = pd.read_csv(\"/content/drive/My Drive/googleColabFiles/Tweets_onefourth.csv\")\n",
        "        sentiment = pd.to_numeric(df.iloc[:, -1], errors=\"raise\", downcast=\"float\")\n",
        "        labels,tweets,sentiment = seperate_tweets(df.iloc[:, 1],hashtags,sentiment)\n",
        "        cleaned_tweets = clean_text(tweets)\n",
        "\n",
        "        y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "\n",
        "        source_idx,target_idx = create_domain_adaptation_index(tweets,labels,sentiment)\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_preprocessed.npz\",tweets=tweets, cleaned_tweets=cleaned_tweets, y=y,sentiment=sentiment,source_idx=source_idx,target_idx=target_idx)\n",
        "        return cleaned_tweets,tweets, y,sentiment,source_idx,target_idx\n",
        "\n",
        "def create_domain_adaptation_index(tweets,labels,sentiment):\n",
        "    labels = np.array([s if \"#bad\" not in s else \"#sad\" for s in labels])\n",
        "    source_idx  = np.array([i for i,val in enumerate(labels) if val== \"#sad\" or val == \"#bad\" ])\n",
        "    target_idx = np.array([i for i,val in enumerate(labels) if val != \"#sad\" and val != \"#bad\" ])\n",
        "    return source_idx,target_idx\n",
        "\n",
        "\n",
        "def main_preprocessing(mode=\"multi_semantic_embedding\"):\n",
        "\n",
        "    # Load neccessary informations about the dataset\n",
        "    cleaned_tweets,tweets,hashtags,sentiment, source_idx, target_idx = load_sentqs_tweets()\n",
        "\n",
        "    if mode == \"multi_semantic_embedding\":\n",
        "\n",
        "        # Obtain embeddings and train deep learning model\n",
        "        model = generate_embedding_model(cleaned_tweets,sentiment,source_idx,target_idx,model_size=\"ftTest\")\n",
        "\n",
        "\n",
        "    elif mode == \"train_embedding\":\n",
        "        #Obtain skipgram embedding only\n",
        "        #Create feature representation: TFIDF-Variants and skipgram embedding with 1000 dimension and negative sampling\n",
        "        # Output will be saved to disk\n",
        "        get_glove_embedding_matrix(cleaned_tweets)\n",
        "        get_skipgram_gensim_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "        # Sentence Skipgram is the base feature representation of the datatset\n",
        "        X = get_skipgram_sentence_embedding_matrix(cleaned_tweets)\n",
        "        create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment)\n",
        "        # Another possible embedding:\n",
        "        Y = get_fastText_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    elif mode == \"describe_dataset\":\n",
        "        # # Describe dataset with some common characteristics\n",
        "        describe_dataset(cleaned_tweets,hashtags)\n",
        "\n",
        "        # ## Plot eigenspectrum of embeddings\n",
        "        #data/\n",
        "        X = np.load(\"sentqs_skipgram_sentence_embedding.npz\",allow_pickle=True)\n",
        "        print(X.files)\n",
        "        plot_eigenspectrum(X['embedding'])\n",
        "\n",
        "        # ## Plot representation of 2 dimensional tsne embedding\n",
        "        plot_tsne(X['embedding'],sentiment)\n",
        "\n",
        "    else:\n",
        "        ## Loads the data into the program and trains machine learning model\n",
        "        load_data_run_classification()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Obtain the all files of the dataset preprocessing, including plots, feature representation etc.\n",
        "    # After running this file you will find the corresponding files for classification in the data folder\n",
        "    # select mode:\n",
        "    main_preprocessing(\"multi_semantic_embedding\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 31215 unique tokens.\n",
            "Shape of data tensor: (15382, 335)\n",
            "Shape of label tensor: (15382, 3)\n",
            "Loaded Skipgram embedding.\n",
            "Loaded Glove embedding.\n",
            "Loaded Skipgram embedding.\n",
            "loaded fastText embedding.\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 331)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_13 (Embedding)     (None, 331, 200)          6243200   \n",
            "_________________________________________________________________\n",
            "lambda_3 (Lambda)            (None, 331, 200, 1)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 327, 196, 128)     3328      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 65, 39, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 61, 35, 128)       409728    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 12, 7, 128)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 3, 128)         409728    \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_2 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 7,082,883\n",
            "Trainable params: 839,683\n",
            "Non-trainable params: 6,243,200\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 331) for input Tensor(\"input_4:0\", shape=(None, 331), dtype=int32), but it was called on an input with incompatible shape (None, 335).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 331) for input Tensor(\"input_4:0\", shape=(None, 331), dtype=int32), but it was called on an input with incompatible shape (None, 335).\n",
            "127/128 [============================>.] - ETA: 0s - loss: 1.0314 - acc: 0.4776WARNING:tensorflow:Model was constructed with shape (None, 331) for input Tensor(\"input_4:0\", shape=(None, 331), dtype=int32), but it was called on an input with incompatible shape (None, 335).\n",
            "128/128 [==============================] - 56s 436ms/step - loss: 1.0315 - acc: 0.4775 - val_loss: 1.0182 - val_acc: 0.5070\n",
            "Epoch 2/2\n",
            "128/128 [==============================] - 56s 435ms/step - loss: 1.0027 - acc: 0.4969 - val_loss: 1.0386 - val_acc: 0.4650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOOi63mz4pI",
        "colab_type": "text"
      },
      "source": [
        "sentqs_demo.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3515sJZ-z8RC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4f0d55e9-103d-43a2-cff5-9f998022b8ee"
      },
      "source": [
        "# Error: default_graph when using only keras and not tensorflow.keras\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Conv2D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "import requests\n",
        "import sys\n",
        "import numpy as np\n",
        "# from NBT import NBT --> already in notebook\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import preprocessing\n",
        "link = \"https://cloud.fhws.de/index.php/s/M4rkbHj9FfW6YKo/download\"\n",
        "# data/\n",
        "file_name = \"/content/drive/My Drive/googleColabFiles/sentqs_dataset.npz\"\n",
        "\n",
        "def download_data():\n",
        "    with open(file_name, \"wb\") as f:\n",
        "            print(\"Downloading %s\" % file_name)\n",
        "            response = requests.get(link, stream=True)\n",
        "            total_length = response.headers.get('content-length')\n",
        "\n",
        "            if total_length is None: # no content length header\n",
        "                f.write(response.content)\n",
        "            else:\n",
        "                dl = 0\n",
        "                total_length = int(total_length)\n",
        "                for data in response.iter_content(chunk_size=4096):\n",
        "                    dl += len(data)\n",
        "                    f.write(data)\n",
        "                    done = int(50 * dl / total_length)\n",
        "                    sys.stdout.write(\"\\r[%s%s]\" % ('=' * done, ' ' * (50-done)) )\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "    data = np.load(file_name,allow_pickle=True)\n",
        "    Xs = data[\"arr_0\"]\n",
        "    Ys = data[\"arr_1\"]\n",
        "    Xt = data[\"arr_2\"]\n",
        "    Yt = data[\"arr_3\"]\n",
        "    return Xs,Ys,Xt,Yt\n",
        "\n",
        "#Xs,Ys,Xt,Yt = download_data()\n",
        "\n",
        "#If dataset file is already downloaded\n",
        "data = np.load(file_name,allow_pickle=True)\n",
        "Xs = data[\"arr_0\"]\n",
        "Ys = data[\"arr_1\"]\n",
        "Xt = data[\"arr_2\"]\n",
        "Yt = data[\"arr_3\"]\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "# Training\n",
        "batch_size = 256\n",
        "epochs = 2\n",
        "# LSTM\n",
        "\n",
        "lstm_output_size = 70\n",
        "Xs = np.expand_dims(Xs, 2)\n",
        "Xt = np.expand_dims(Xt, 2)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "model.add(MaxPooling1D(pool_size=pool_size))\n",
        "model.add(LSTM(lstm_output_size))\n",
        "model.add(Dense(35))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "# model.summary()\n",
        "print('Train...')\n",
        "model.fit(Xs, Ys,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(Xt, Yt))\n",
        "score, acc = model.evaluate(Xt, Yt, batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train...\n",
            "Epoch 1/2\n",
            "84/84 [==============================] - 3s 35ms/step - loss: -26.6878 - accuracy: 0.1853 - val_loss: 14.8169 - val_accuracy: 0.6096\n",
            "Epoch 2/2\n",
            "84/84 [==============================] - 2s 27ms/step - loss: -117.8562 - accuracy: 0.1861 - val_loss: 42.1653 - val_accuracy: 0.6096\n",
            "157/157 [==============================] - 1s 6ms/step - loss: 42.1653 - accuracy: 0.6096\n",
            "Test score: 42.165313720703125\n",
            "Test accuracy: 0.6096078157424927\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}