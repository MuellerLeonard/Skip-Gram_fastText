{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiments.ipynb",
      "provenance": [],
      "mount_file_id": "1_TNWvEiS5H1OyvP4rcZdEdOSJ_HferoX",
      "authorship_tag": "ABX9TyPLI80QrRWGLbSs/qYlNN8u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuellerLeonard/Word-embeddings_CNN/blob/master/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFxZi-vfiLZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b414aa57-4e91-4e67-9d10-d1fecee9dfdd"
      },
      "source": [
        "!pip install tensorflow-gpu\n",
        "!pip install keras\n",
        "!pip install requests\n",
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/11/763f55d3d15efd778ef24453f126e6c33635680e5a2bb346da3fab5997cb/tensorflow_gpu-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.10.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.3.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.35.1)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (50.3.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu) (3.1.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.3.0\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3017908 sha256=c36aaa72d05e0598e9c8b5bc9b55b93f40dde55b1702f5c1a401d6b4b196810c\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5-NX3_k_Ve",
        "colab_type": "text"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akkPpgAbjA38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2f44fc0b-9fc6-4fb7-d558-94822f3b08da"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from typing import List\n",
        "import nltk\n",
        "from nltk import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(tweets: List[str], lang: str = 'english') -> List[str]:\n",
        "    \"\"\"\n",
        "    performes stemming and other \"cleanup\"\n",
        "    :param tweets:\n",
        "    :param lang:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    data: List[str] = tweets\n",
        "    cleaned_text = []\n",
        "    stops = set(stopwords.words(lang))\n",
        "\n",
        "    # no ! and .\n",
        "    table = str.maketrans(dict.fromkeys(\"\"\"\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\"\"\"))\n",
        "\n",
        "    for text in data:\n",
        "        text = re.sub(r'https?://[^\\s]*', ' tokenlink ', text)\n",
        "        text = re.sub(r'\\.\\.\\.', ' tokendotdotdot ', text)\n",
        "        text = re.sub(r'xD', ' tokenxd ', text)\n",
        "        text = re.sub(r':\\)', ' tokenxbrackethappy ', text)\n",
        "        text = re.sub(r':\\(', ' tokenxbracketsad ', text)\n",
        "        text = re.sub(r':-\\)', ' tokennosehappy ', text)\n",
        "        text = re.sub(r':-\\(', ' tokennosesad ', text)\n",
        "        text = re.sub(r':D', ' tokenxcheer ', text)\n",
        "        text = re.sub(r':-S', ' tokens ', text)\n",
        "\n",
        "\n",
        "        ## Convert words to lower case and split them\n",
        "        text = text.lower().split()\n",
        "\n",
        "        text = \" \".join(text)\n",
        "\n",
        "        # Clean the text\n",
        "        text = re.sub(r\"(?<!\\.)\\.(?!\\.)\", \" \", text)\n",
        "        text = re.sub(r\"[^A-Za-z0-9^,!./'+-=]\", \" \", text)\n",
        "        text = re.sub(r\"what's\", \"what is \", text)\n",
        "        text = re.sub(r\"\\'s\", \" \", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "        text = re.sub(r\"n't\", \" not \", text)\n",
        "        text = re.sub(r\"i'm\", \"i am \", text)\n",
        "        text = re.sub(r\"\\'re\", \" are \", text)\n",
        "        text = re.sub(r\"\\'d\", \" would \", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "        text = re.sub(r\",\", \" \", text)\n",
        "        text = re.sub(r\"\\.\", \" \", text)\n",
        "        text = re.sub(r\"!\", \" ! \", text)\n",
        "        text = re.sub(r\"/\", \" \", text)\n",
        "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "        text = re.sub(r\"\\+\", \" + \", text)\n",
        "        text = re.sub(r\"-\", \" - \", text)\n",
        "        text = re.sub(r\"=\", \" = \", text)\n",
        "        text = re.sub(r\"'\", \" \", text)\n",
        "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "        text = re.sub(r\":\", \" : \", text)\n",
        "        text = re.sub(r\" e g \", \" eg \", text)\n",
        "        text = re.sub(r\" b g \", \" bg \", text)\n",
        "        text = re.sub(r\" u s \", \" american \", text)\n",
        "        text = re.sub(r\"\\0s\", \"0\", text)\n",
        "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "        text = re.sub(r\"e - mail\", \"email\", text)\n",
        "        text = re.sub(r\"j k\", \"jk\", text)\n",
        "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "        ## Remove puncuation\n",
        "        text = text.translate(table)\n",
        "\n",
        "        text = text.split()\n",
        "\n",
        "        ## Remove stop words[^\\s][^\\s]\n",
        "        #text = [w for w in text if not w in stops]\n",
        "\n",
        "        stemmer = SnowballStemmer(lang)\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "        cleaned_text.append(text)\n",
        "\n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKEDtAVZjpE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "72cb4689-de1c-4864-b4ca-5cd260990ea8"
      },
      "source": [
        "import scipy.io as sio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "# import cleanup --> already in notebook\n",
        "import keras\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from tensorflow.keras.layers import Dense, Embedding, Flatten, Input, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams, pad_sequences\n",
        "from sklearn.manifold import TSNE\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn import decomposition\n",
        "#from keras_preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import make_sampling_table\n",
        "from numpy import asarray, zeros\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling3D, GlobalMaxPooling2D, MaxPooling2D, GlobalAveragePooling2D,BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, Conv3D, Conv2D, MaxPooling3D, Embedding, concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.initializers import Constant\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from scipy import spatial\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import backend\n",
        "import tensorflow as tf\n",
        "# fastText\n",
        "#import fasttext.util\n",
        "import fasttext\n",
        "from fasttext import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from io import StringIO\n",
        "import math\n",
        "from gensim.models import FastText\n",
        "# 1DConv Net for comparison\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D,Conv2D\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "\n",
        "def load_data_run_classification():\n",
        "  # 'data/....npz'\n",
        "    data = np.load('/content/drive/My Drive/googleColabFiles/sentqs_dataset.npz')\n",
        "    Xs = data[\"arr_0\"]\n",
        "    Ys = data[\"arr_1\"]\n",
        "    Xt = data[\"arr_2\"]\n",
        "    Yt = data[\"arr_3\"]\n",
        "    print(\"Classification Task Test \\n\")\n",
        "    from sklearn.ensemble import GradientBoostingClassifier\n",
        "    clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Xs, Ys)\n",
        "    print(clf.score(Xt, Yt))\n",
        "\n",
        "def create_tfidf(sen,min_df=10,max_df=100):\n",
        "    print(\"Create TF-IDF\\n\")\n",
        "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df)\n",
        "    X = vectorizer.fit_transform(sen)\n",
        "    return X.toarray()\n",
        "\n",
        "def save_dataset(X, y, prefix =\"\"):\n",
        "    print(\"Save Dataset \\n\")\n",
        "    y = np.array(y)[:, None]\n",
        "    dataset = np.concatenate([y,X], axis=1)\n",
        "\n",
        "    # data/\n",
        "    np.save(\"sentqs_da_\"+str(prefix)+\".npy\",dataset)\n",
        "    return dataset\n",
        "\n",
        "def seperate_tweets(data,hashtags,sentiment):\n",
        "    print(\"Seperate Tweets \\n\")\n",
        "    labels = []\n",
        "    tweets = []\n",
        "    sentiment_new = []\n",
        "    for t,s in zip(data,sentiment):\n",
        "        for h in hashtags:\n",
        "            t = t.lower()\n",
        "            h = \"#\" + h.lower()\n",
        "            if h in t:\n",
        "                labels.append(h)\n",
        "                tweets.append(t.replace(h,\" \"))\n",
        "                sentiment_new.append(s)\n",
        "                break\n",
        "\n",
        "    return labels,tweets,sentiment_new\n",
        "\n",
        "def generate_data(corpus, window_size, V):\n",
        "    for words in corpus:\n",
        "        couples, labels = skipgrams(words, V, window_size, negative_samples=1, shuffle=True,sampling_table=make_sampling_table(V, sampling_factor=1e-05))\n",
        "        if couples:\n",
        "            X, y = zip(*couples)\n",
        "            X = np_utils.to_categorical(X, V)\n",
        "            y = np_utils.to_categorical(y, V)\n",
        "            yield X, y\n",
        "\n",
        "def get_glove_embedding_matrix(texts, dim=200):\n",
        "    # data/\n",
        "    if os.path.isfile(\"sentqs_glove_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_glove_embedding.npz\")\n",
        "        print('Loaded Glove embedding.')\n",
        "        return loaded_embedding['embedding']\n",
        "    else:\n",
        "        # first, build index mapping words in the embeddings set\n",
        "        # to their embedding vector\n",
        "\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "        sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "        word_index = tokenizer.word_index\n",
        "\n",
        "        print('Indexing word vectors.')\n",
        "\n",
        "        embeddings_index = {}\n",
        "        # data/\n",
        "        with open('/content/drive/My Drive/googleColabFiles/dataset/glove.twitter.27B.200d.txt', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                word, coefs = line.split(maxsplit=1)\n",
        "                coefs = np.fromstring(coefs, 'f', sep=' ')\n",
        "                embeddings_index[word] = coefs\n",
        "\n",
        "        print('Found %s word vectors.' % len(embeddings_index))\n",
        "        print('Preparing embedding matrix.')\n",
        "\n",
        "        # prepare embedding matrix\n",
        "        num_words = len(word_index) + 1\n",
        "        embedding_matrix = np.zeros((num_words, dim))\n",
        "        counter = 0\n",
        "        for word, i in word_index.items():\n",
        "            # if i >= MAX_NUM_WORDS:\n",
        "            #    counter +=1\n",
        "            #    continue\n",
        "            embedding_vector = embeddings_index.get(word)\n",
        "            if embedding_vector is not None:\n",
        "                # words not found in embedding index will be all-zeros.\n",
        "                embedding_matrix[i] = embedding_vector\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_glove_embedding.npz\", embedding=embedding_matrix)\n",
        "        return embedding_matrix\n",
        "\n",
        "def get_fastText_embedding_matrix(texts, BASE_DIR = '', MAX_SEQUENCE_LENGTH = 1000, EMBEDDING_DIM = 200):\n",
        "# dataset dimension must be 200\n",
        "    FT_MODEL = \"/content/drive/My Drive/googleColabFiles/dataset/cc.en200.bin\"\n",
        "    \n",
        "    #code for reducing dimensions of file cc.en200.bin, path needs to be adjusted\n",
        "    #FT_MODEL =fasttext.load_model('/content/drive/My Drive/googleColabFiles/dataset/cc.en.300.bin')\n",
        "    #print(\"loading model\")\n",
        "    #fasttext.util.reduce_model(FT_MODEL, EMBEDDING_DIM)\n",
        "    #print(\"dimensions reduced\")\n",
        "\n",
        "    #FT_MODEL.get_dimension()\n",
        "\n",
        "    if os.path.isfile(\"sentqs_fastText_embedding.npz\"):\n",
        "      saved_embedding = np.load(\"sentqs_fastText_embedding.npz\")\n",
        "      print('loaded pre_trained_fastText embedding.')\n",
        "      return saved_embedding['embedding']\n",
        "\n",
        "    else:\n",
        "      #obtain pretrained vectors:\n",
        "      print('Indexing word vectors.')\n",
        "\n",
        "      filename, file_extension = os.path.splitext(FT_MODEL)\n",
        "\n",
        "      if file_extension == '.vec':\n",
        "          embeddings_index = {}\n",
        "          with open(os.path.join(FT_MODEL)) as f:\n",
        "              for line in f:\n",
        "                  values = line.split()\n",
        "                  word = values[0]\n",
        "                  coefs = np.asarray(values[1:], dtype='float32')\n",
        "                  embeddings_index[word] = coefs\n",
        "            \n",
        "          print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "      # use pretrained model on tweets:\n",
        "      print('Found %s texts.' % len(texts))\n",
        "      # finally, vectorize the text samples into a 2D integer tensor\n",
        "      # num_words = size of vocab\n",
        "      tokenizer = Tokenizer()\n",
        "      tokenizer.fit_on_texts(texts)\n",
        "      sequences = tokenizer.texts_to_sequences(texts)\n",
        "      word_index = tokenizer.word_index\n",
        "      print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "      print('Preparing embedding matrix.')\n",
        "\n",
        "      # load the fasttext model\n",
        "      f = load_model(FT_MODEL)\n",
        "\n",
        "      # prepare embedding matrix\n",
        "      num_words = len(word_index) + 1\n",
        "      emb_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "\n",
        "      for word, i in word_index.items():\n",
        "        # if i >= MAX_NUM_WORDS:\n",
        "          # continue\n",
        "        # embedding_vector = embeddings_index.get(word)\n",
        "          embedding_vector = f.get_word_vector(word)\n",
        "          if embedding_vector is not None:\n",
        "              # words not found in embedding index will be all-zeros.\n",
        "              emb_matrix[i] = embedding_vector\n",
        "\n",
        "      np.savez_compressed(\"sentqs_fastText_embedding.npz\", embedding = emb_matrix)\n",
        "      return emb_matrix\n",
        "\n",
        "def get_fastText_sentence_embedding_matrix(text, dim=200):\n",
        "\n",
        "    if os.path.isfile(\"sentqs_fastText_sentence_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_fastText_sentence_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded fastText_sentence embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        text = [''.join(x) for x in text]\n",
        "\n",
        "        #create .txt file from cleaned_tweets\n",
        "        df = pd.DataFrame(text)\n",
        "        df.to_csv(r'text.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "        #train unsupervised model with skipgram\n",
        "        modelft = fasttext.train_unsupervised('text.txt', \"skipgram\", dim=dim)\n",
        "        #Model can be saved:\n",
        "        #modelft.save_model(\"textmodel.txt\")\n",
        "\n",
        "        #t = Tokenizer()\n",
        "        #t.fit_on_texts(text)\n",
        "\n",
        "        #get each sentence vector from fastText Model and create a matrix\n",
        "        i = 0\n",
        "        sent_matrix = np.zeros((len(text), dim))\n",
        "        for sentence in text:\n",
        "          sentence_vector = modelft.get_sentence_vector(sentence)\n",
        "          if sentence_vector is not None:\n",
        "            sent_matrix[i] = sentence_vector\n",
        "            i = i + 1\n",
        "            if i == len(text):\n",
        "              break\n",
        "\n",
        "        #print(\"sentence_matrix: \", sent_matrix)\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_fastText_sentence_embedding\", embedding=sent_matrix)\n",
        "        return sent_matrix   \n",
        "\n",
        "# epochs 100 to 10\n",
        "# batch_size, epochs depends on how much input data there is\n",
        "def get_skipgram_sentence_embedding_matrix(text, dim=200, batch_size=64, window_size=5, epochs = 1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_sentence_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_sentence_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        text = [''.join(x) for x in text]\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "        corpus = t.texts_to_sequences(text)\n",
        "        #print(corpus)\n",
        "        V = len(t.word_index)\n",
        "        step_size = len(corpus) // batch_size\n",
        "        model = Sequential()\n",
        "        model.add(Dense(dim, input_dim=V, activation=\"softmax\"))\n",
        "        model.add(Dense(V, input_dim=dim, activation='softmax'))\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "        model.summary()\n",
        "\n",
        "        model.fit(generate_data(corpus, window_size, V), epochs=epochs, steps_per_epoch=step_size)\n",
        "        # model.save(\"data/sentqs_full_skigram_arc.h5\")\n",
        "        mlb = MultiLabelBinarizer()\n",
        "        print(\"step 1\")\n",
        "        enc = mlb.fit_transform(corpus)\n",
        "        print(\"step 2\")\n",
        "        emb = enc @ model.get_weights()[0]\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_sentence_embedding\", embedding=emb)\n",
        "        return emb\n",
        "\n",
        "def  get_skipgram_gensim_embedding_matrix(text, dim = 200, window_size=5, min_word_occurance=1, epochs=1):\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_skipgram_gensim_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_skipgram_gensim_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded Skipgram embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        x = [row.split(' ') for row in text]\n",
        "        model = Word2Vec(x, size=dim, window=window_size, min_count=min_word_occurance, workers=4, sg=1) #sg = 1: use skipgram\n",
        "\n",
        "        words = model.wv.vocab.keys()\n",
        "        vocab_size = len(words)\n",
        "        print(\"Vocab size\", vocab_size)\n",
        "\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "\n",
        "        # total vocabulary size plus 0 for unknown words\n",
        "        # vocab_size = len(vocab) + 1\n",
        "        # define weight matrix dimensions with all 0\n",
        "        weight_matrix = zeros((vocab_size, dim))\n",
        "        # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "        for word, i in t.word_index.items():\n",
        "            if i > vocab_size: break\n",
        "            if word in model.wv.vocab.keys():\n",
        "                weight_matrix[i] = model.wv[word]\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_skipgram_gensim_embedding\", embedding=weight_matrix)\n",
        "        return weight_matrix\n",
        "\n",
        "def get_fastText_gensim_embedding_matrix(text, dim = 200, word_ngrams=1, min_char_ngrams=3, max_char_ngrams=6, window_size=5, min_word_occurance=1, epochs=10):\n",
        "    if os.path.isfile(\"sentqs_fastText_gensim_embedding.npz\"):\n",
        "      # data/\n",
        "        loaded_embedding = np.load(\"sentqs_fastText_gensim_embedding.npz\")\n",
        "        loaded_embedding = loaded_embedding[\"embedding\"]\n",
        "        print('Loaded fastText_gensim embedding.')\n",
        "        return loaded_embedding\n",
        "    else:\n",
        "        x = [row.split(' ') for row in text]\n",
        "        modelft = FastText(size=dim, word_ngrams=word_ngrams,min_n=min_char_ngrams, max_n=max_char_ngrams, window=window_size, min_count=min_word_occurance, workers=4) \n",
        "        modelft.build_vocab(sentences=x)\n",
        "        modelft.train(sentences=x, total_examples=len(x), epochs=epochs) #train\n",
        "\n",
        "        words = modelft.wv.vocab.keys()\n",
        "        vocab_size = len(words)\n",
        "        print(\"Vocab size\", vocab_size)\n",
        "\n",
        "        t = Tokenizer()\n",
        "        t.fit_on_texts(text)\n",
        "\n",
        "        # total vocabulary size plus 0 for unknown words\n",
        "        # vocab_size = len(vocab) + 1\n",
        "        # define weight matrix dimensions with all 0\n",
        "        ft_weight_matrix = zeros((vocab_size, dim))\n",
        "        # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "        for word, i in t.word_index.items():\n",
        "            if i > vocab_size: break\n",
        "            if word in modelft.wv.vocab.keys():\n",
        "                ft_weight_matrix[i] = modelft.wv[word]\n",
        "\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_fastText_gensim_embedding\", embedding=ft_weight_matrix)\n",
        "        return ft_weight_matrix\n",
        "\n",
        "\n",
        "def print_results(N, p, r):\n",
        "    print(\"N\\t\" + str(N))\n",
        "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
        "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
        "\n",
        "\n",
        "def text_classification_ft(text, sentiment, train_size=0.8, epochs=25, learningrate=1.0, word_n_grams=2, dim=50, bucket=200000, loss='hs'):\n",
        "    #sentiment: 2.0 = negative, 1.0 = positive, 0.0 = neutral\n",
        "    #supervised fastText only for Tweets.csv\n",
        "    #does not have to be Tokenized\n",
        "    \n",
        "    #converting float values of sentiment to string values in a new column\n",
        "    score = [sentiment.size]\n",
        "    for x in sentiment:\n",
        "        if x == 0.0:\n",
        "          score.append(\"neutral\")  \n",
        "        elif x == 1.0:\n",
        "          score.append(\"positive\")\n",
        "        elif x == 2.0:\n",
        "          score.append(\"negative\")\n",
        "        \n",
        "    score.pop(0)\n",
        "\n",
        "    print(len(score))\n",
        "    print(score) \n",
        "\n",
        "    #Preprocessing\n",
        "    #combining individual columns to a table\n",
        "    texts = [''.join(x) for x in text]\n",
        "    df = pd.DataFrame(texts)\n",
        "    dfs = pd.DataFrame(sentiment)\n",
        "    dfsc = pd.DataFrame(score)\n",
        "    combined = pd.concat([dfs, df, dfsc], axis=1)\n",
        "    combined.columns = ['sentiment', 'text', 'score']\n",
        "    combined.to_csv('tweets.csv', index=False)\n",
        "    tweets = pd.read_csv(\"tweets.csv\")\n",
        "    \n",
        "    print(tweets.head())\n",
        "\n",
        "    #formatting the data-table so that fastText can use it\n",
        "    col = ['score', 'text']\n",
        "\n",
        "    tweets = tweets[col]\n",
        "    tweets['score']=['__label__'+ s for s in tweets['score']]\n",
        "    tweets['text']= tweets['text'].replace('\\n',' ', regex=True).replace('\\t',' ', regex=True)\n",
        "    tweets.to_csv(r'tweets_updated.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "    print(\"updated dataframe: \")\n",
        "    print(tweets.head())\n",
        "    train_data = math.ceil(len(texts) * train_size)\n",
        "    test_data = math.floor(len(texts) * (1-train_size))\n",
        "  \n",
        "    #split data into head and tail for train and test\n",
        "    print(\"splitting data into training and test\", train_data, \",\", test_data)\n",
        "    tweets_train = tweets.head(train_data)\n",
        "    tweets_train.to_csv(r'tweets_train.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "    tweets_test = tweets.tail(test_data)\n",
        "    tweets_test.to_csv(r'tweets_test.txt', index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n",
        "\n",
        "    print(\"train fastText Model supervised with the training data: \")\n",
        "    # Hyperparameters can be optimized, using the Standard ones\n",
        "    modelft = fasttext.train_supervised(input=\"tweets_train.txt\")#, lr=learningrate, epoch=epochs, wordNgrams=word_n_grams, bucket=bucket, dim=dim, loss=loss)\n",
        "    print(\"words: \", modelft.words)\n",
        "    print(\"labels: \", modelft.labels)\n",
        "    print(\"Precision and Recall on test data: \")\n",
        "    print_results(*modelft.test(\"tweets_test.txt\"))\n",
        "\n",
        "    print(\"predict on a sentence and return the probability of the labels: \")\n",
        "    sentence = \"i had a bad injury a few months ago, but now i am finally well\"\n",
        "    cleaned_sentence = clean_text(sentence)\n",
        "    print(\"sentence: \", cleaned_sentence)\n",
        "    print(modelft.predict(sentence, k=3))\n",
        "\n",
        "def generate_embedding_model(text, y,source_idx,target_idx,batch_size=32, epochs = 50, save = True, dim = 200, val_split=0.2,model_size=\"1D\"):\n",
        "    # Preprocessing\n",
        "    #MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    MAX_SEQUENCE_LENGTH = 335\n",
        "    texts = [''.join(x) for x in text]\n",
        "    # finally, vectorize the text samples into a 2D integer tensor\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    labels = to_categorical(np.asarray(y))\n",
        "    print('Shape of data tensor:', data.shape)\n",
        "    print('Shape of label tensor:', labels.shape)\n",
        "    num_words = len(word_index) + 1\n",
        "    #for sentence classification\n",
        "    num_sentences = len(text)\n",
        "\n",
        "    # split the data into a training set and a validation set\n",
        "    indices = np.arange(data.shape[0])\n",
        "    print(\"indices: \", indices.shape)\n",
        "    np.random.shuffle(indices)\n",
        "    data = data[indices]\n",
        "    print(\"data: \", data.shape)\n",
        "    #print(\"data: \", data)\n",
        "    labels = labels[indices]\n",
        "    num_validation_samples = int(val_split * data.shape[0])\n",
        "\n",
        "    x_train = data[source_idx]\n",
        "    y_train = labels[source_idx]\n",
        "    x_val = data[target_idx]\n",
        "    y_val = labels[target_idx]\n",
        "\n",
        "    #for sentence classification:\n",
        "    #1D array, mapping one label to each sentence, like word classification only for sentences --> very clunky, but how i understood it\n",
        "    #each sentence will get a vector from the sentence_embedding_matrix\n",
        "    #accuracy around 60%\n",
        "\n",
        "    number_sentences = len(text)\n",
        "    matrix = np.arange(number_sentences)\n",
        "    matrix = matrix[indices]\n",
        "    w_train = matrix[source_idx]\n",
        "    v_train = labels[source_idx]\n",
        "    w_val = matrix[target_idx]\n",
        "    v_val = labels[target_idx]\n",
        "\n",
        "    # not sure\n",
        "    emb = get_skipgram_gensim_embedding_matrix(text, epochs=1)\n",
        "    emb = np.expand_dims(emb, 1)\n",
        "    emb_train = emb[:-num_validation_samples]\n",
        "    emb_val = emb[-num_validation_samples:]\n",
        "    # Build model\n",
        "    MAX_SEQUENCE_LENGTH = len(max(text, key=lambda i: len(i))) + 1\n",
        "    with tf.device('/GPU:0'):\n",
        "        #sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name=\"embedding_input\")\n",
        "\n",
        "        glove_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_glove_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)#(sequence_input)\n",
        "        skipgram_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_skipgram_gensim_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)#(sequence_input)\n",
        "        #fastText emmbedding layer:\n",
        "        #pre-Trained model: get_fastText_embedding_matrix\n",
        "        #only trained on Tweets.csv: get_fastText_gensim_embedding_matrix \n",
        "        fastText_embedding_layer = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    weights=[get_fastText_gensim_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)\n",
        "        #for sentence classification: \n",
        "        #FastText: get_fastText_sentence_embedding_matrix \n",
        "        #Skip-gram: get_skipgram_sentence_embedding_matrix\n",
        "        sentence_embedding_layer = Embedding(num_sentences,\n",
        "                                    dim,\n",
        "                                    weights=[get_fastText_sentence_embedding_matrix(texts, dim)],\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=False)\n",
        "        \n",
        "        #Training with 2 embedding layers at a time\n",
        "        if model_size ==\"2D\":\n",
        "\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = fastText_embedding_layer(sequence_input)\n",
        "            embedded_sequences1 = skipgram_embedding_layer(sequence_input)\n",
        "            #FastText: embedded_sequences\n",
        "            #Glove: glove_embedding_layer\n",
        "            #Skipgram: skipgram_embedding_layer\n",
        "            combined=  tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([embedded_sequences, embedded_sequences1])\n",
        "            x = Conv2D(128, 5, activation='relu')(combined)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = MaxPooling2D(5)(x)\n",
        "            x = Conv2D(128, 5, activation='relu')(x)\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "            x = Dense(128, activation='relu')(x)\n",
        "\n",
        "        #Model for sentence classification training --> bad\n",
        "        #kernel size must be 1 and pool_size as well, cause given array is one dimensional     \n",
        "        if model_size ==\"sentence\":\n",
        "\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = sentence_embedding_layer(sequence_input)\n",
        "\n",
        "            x = Conv1D(64, 1, padding='valid', activation='relu', strides=1) (embedded_sequences)\n",
        "            x = MaxPooling1D(pool_size=1) (x)\n",
        "            x = LSTM(70) (x)\n",
        "            x = Dense(35) (x)\n",
        "            x = Dense(1) (x)\n",
        "            x = Activation('sigmoid') (x)\n",
        "\n",
        "            preds = Dense(3, activation='softmax')(x)\n",
        "            model = Model(inputs=sequence_input, outputs=preds)\n",
        "\n",
        "            model.compile(loss='binary_crossentropy',\n",
        "                        optimizer='adam',\n",
        "                        metrics=['accuracy'])\n",
        "            \n",
        "            model.summary()\n",
        "            \n",
        "            model.fit(w_train, v_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(w_val, v_val))\n",
        "            return model\n",
        "\n",
        "        #1DConv Model\n",
        "        #training with one embedding layer at a time\n",
        "        #fastText_embedding_layer:\n",
        "        #glove_embedding_layer:\n",
        "        #skipgram_embedding_layer:\n",
        "        if model_size ==\"1D\":\n",
        "            sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "            embedded_sequences = fastText_embedding_layer(sequence_input)\n",
        "\n",
        "            x = Conv1D(64, 5, padding='valid', activation='relu', strides=1) (embedded_sequences)\n",
        "            x = MaxPooling1D(pool_size=4) (x)\n",
        "            x = LSTM(70) (x)\n",
        "            x = Dense(35) (x)\n",
        "            x = Dense(1) (x)\n",
        "            x = Activation('sigmoid') (x)\n",
        "\n",
        "        #not used, would be for 3 embeddings at a time\n",
        "        if model_size ==\"large\":\n",
        "            skipgram_sentence_embedding = Embedding(num_words,\n",
        "                                    dim,\n",
        "                                    #embeddings_initializer=Constant(get_skipgram_gensim_embedding_matrix(text, epochs=1)),\n",
        "                                    #weights=get_skipgram_gensim_embedding_matrix(text, epochs=1),\n",
        "                                    input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                    trainable=True)(sequence_input)\n",
        "\n",
        "            combined = tf.keras.layers.Lambda(lambda t: tf.stack(t,axis=3))([skipgram_embedding_layer, glove_embedding_layer,skipgram_sentence_embedding])\n",
        "            x = DenseNet121(include_top=False, weights=None, input_shape = (MAX_SEQUENCE_LENGTH, dim, 3))(combined)\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        #compiling Model\n",
        "        preds = Dense(3, activation='softmax')(x)\n",
        "        model = Model(inputs=sequence_input, outputs=preds)\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer='rmsprop',\n",
        "                    metrics=['acc'])\n",
        "\n",
        "        model.summary()\n",
        "        # plot_model(model, to_file='model_combined.png')\n",
        "\n",
        "        # Train model\n",
        "        model.fit(x_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(x_val, y_val))\n",
        "        \n",
        "        # scores = model.evaluate(x_val, y_val)\n",
        "        # print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "        if save:\n",
        "          # data/\n",
        "            model.save(\"sentqs_full.h5\")\n",
        "        return model\n",
        "\n",
        "def tsne_embedding(X):\n",
        "    print(\"Starting TSNE\\n\")\n",
        "    for p in [5,25,50,75,100]:\n",
        "        tsne = TSNE(n_components=2, init='random',\n",
        "             random_state=0, perplexity=p)\n",
        "        xl = tsne.fit_transform(X)\n",
        "        # data/\n",
        "        np.save(\"sentqs_tsne_\"+str(p)+\".npy\",xl)\n",
        "        print(\"Finished TSNE\\n\")\n",
        "\n",
        "\n",
        "def describe_dataset(tweets,labels):\n",
        "    data = pd.DataFrame([tweets, labels]).T\n",
        "    description = data.describe()\n",
        "    print(description)\n",
        "\n",
        "    print(\"Class Counts:\")\n",
        "    class_counts = data.groupby(1).size()\n",
        "\n",
        "    x = class_counts.to_numpy()\n",
        "    keys = class_counts.keys().to_list()\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.bar(keys, x)\n",
        "    plt.ylabel(\"Tweet Count\")\n",
        "    plt.xticks(range(len(keys)), keys, rotation=45)\n",
        "    plt.xlabel(\"Hashtags\")\n",
        "    plt.tight_layout()\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_class_dist.pdf\", dpi=1000, transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_eigenspectrum(x):\n",
        "    values = np.linalg.svd(x,compute_uv=False)\n",
        "    plt.bar(range(101), values[:101], align='center')\n",
        "    plt.ylabel(\"Eigenvalue\")\n",
        "    # plt.tight_layout()\n",
        "    plt.xlabel(\"No.\")\n",
        "    plt.xticks([0, 20, 40, 60, 80, 100], [1, 20, 40, 60, 80, 100])\n",
        "    # plots/\n",
        "    plt.savefig(\"sentqs_spectra.pdf\", transparent=True)\n",
        "    plt.show()\n",
        "\n",
        "# Y for fastText\n",
        "def plot_tsne(X:None,labels):\n",
        "    tsne_embedding(X)\n",
        "\n",
        "    y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "    for p in [5, 25, 50, 75, 100]:\n",
        "      # data/\n",
        "        d = np.load(\"sentqs_tsne_\" + str(p) + \".npy\")\n",
        "        for idx, l in enumerate(list(set(labels))):\n",
        "            c = np.where(y == idx)[0]\n",
        "            x = d[c, :]\n",
        "            plt.scatter(x[:, 0], x[:, 1], s=.5, label=l)\n",
        "            plt.legend(markerscale=10., bbox_to_anchor=(1, 1.02))\n",
        "        plt.ylabel(\"$x_1$\")\n",
        "        plt.xlabel(\"$x_2$\")\n",
        "        plt.tight_layout()\n",
        "        # plots/\n",
        "        plt.savefig('sentqs_tsne_plot_' + str(p) + \".pdf\", dpi=1000, transparent=True)\n",
        "        plt.show()\n",
        "\n",
        "def create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment):\n",
        "\n",
        "    Xs = X[source_idx]\n",
        "    Xt = X[target_idx]\n",
        "    Ys = sentiment[source_idx]\n",
        "    Yt = sentiment[target_idx]\n",
        "    data = [Xs,Ys,Xt,Yt]\n",
        "    # data/\n",
        "    np.savez('sentqs_dataset.npz', *data)\n",
        "    # data/\n",
        "    sio.savemat('sentqs_dataset.mat', {'Xs': Xs, 'Xt': Xt, 'Ys': Ys, 'Yt': Yt})\n",
        "    source_tweets = [tweets[i] for i in source_idx]\n",
        "    target_tweets = [tweets[i] for i in target_idx]\n",
        "\n",
        "    # data/\n",
        "    pd.DataFrame(source_tweets).to_csv(\"sentqs_source_tweets.csv\")\n",
        "    # data/\n",
        "    pd.DataFrame(target_tweets).to_csv(\"sentqs_target_tweets.csv\")\n",
        "    return  Xs,Ys,Xt,Yt\n",
        "\n",
        "def load_sentqs_tweets():\n",
        "  # data/\n",
        "    if os.path.isfile(\"sentqs_preprocessed.npz\"):\n",
        "      # data/\n",
        "        loaded_data = np.load(\"sentqs_preprocessed.npz\")\n",
        "        return loaded_data['cleaned_tweets'],loaded_data[\"tweets\"], loaded_data['y'],loaded_data['sentiment'],loaded_data[\"source_idx\"],loaded_data[\"target_idx\"]\n",
        "    else:\n",
        "        hashtags = ['ADBE', 'GOOGL', 'AMZN', 'AAPL', 'ADSK', 'BKNG', 'EXPE', 'INTC', 'MSFT', 'NFLX', 'NVDA', 'PYPL', 'SBUX',\n",
        "         'TSLA', 'XEL', 'positive', 'bad', 'sad']\n",
        "\n",
        "        # Loading and preprocessing of tweets\n",
        "        # only using a small part of available Tweets to not overload the ram\n",
        "        df = pd.read_csv(\"/content/drive/My Drive/googleColabFiles/Tweets.csv\")\n",
        "        sentiment = pd.to_numeric(df.iloc[:, -1], errors=\"raise\", downcast=\"float\")\n",
        "        labels,tweets,sentiment = seperate_tweets(df.iloc[:, 1],hashtags,sentiment)\n",
        "        cleaned_tweets = clean_text(tweets)\n",
        "\n",
        "        y = preprocessing.LabelEncoder().fit_transform(labels)\n",
        "\n",
        "        source_idx,target_idx = create_domain_adaptation_index(tweets,labels,sentiment)\n",
        "        # data/\n",
        "        np.savez_compressed(\"sentqs_preprocessed.npz\",tweets=tweets, cleaned_tweets=cleaned_tweets, y=y,sentiment=sentiment,source_idx=source_idx,target_idx=target_idx)\n",
        "        return cleaned_tweets,tweets, y,sentiment,source_idx,target_idx\n",
        "\n",
        "def create_domain_adaptation_index(tweets,labels,sentiment):\n",
        "    labels = np.array([s if \"#bad\" not in s else \"#sad\" for s in labels])\n",
        "    source_idx  = np.array([i for i,val in enumerate(labels) if val== \"#sad\" or val == \"#bad\" ])\n",
        "    target_idx = np.array([i for i,val in enumerate(labels) if val != \"#sad\" and val != \"#bad\" ])\n",
        "    return source_idx,target_idx\n",
        "\n",
        "\n",
        "def main_preprocessing(mode=\"multi_semantic_embedding\"):\n",
        "\n",
        "    # Load neccessary informations about the dataset\n",
        "    cleaned_tweets,tweets,hashtags,sentiment, source_idx, target_idx = load_sentqs_tweets()\n",
        "\n",
        "    if mode == \"multi_semantic_embedding\":\n",
        "\n",
        "        # Obtain embeddings and train deep learning model\n",
        "        model = generate_embedding_model(cleaned_tweets,sentiment,source_idx,target_idx,model_size=\"1D\")\n",
        "        # supervised training and testing with fastText on Tweets.csv:\n",
        "        # text_classification_ft(cleaned_tweets, sentiment)\n",
        "\n",
        "    elif mode == \"train_embedding\":\n",
        "        #Obtain skipgram embedding only\n",
        "        #Create feature representation: TFIDF-Variants and skipgram embedding with 1000 dimension and negative sampling\n",
        "        # Output will be saved to disk\n",
        "        get_glove_embedding_matrix(cleaned_tweets)\n",
        "        get_skipgram_gensim_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "        # Sentence Skipgram is the base feature representation of the datatset\n",
        "        X = get_skipgram_sentence_embedding_matrix(cleaned_tweets)\n",
        "        create_domain_adaptation_dataset(X,tweets,source_idx,target_idx,sentiment)\n",
        "        # Another possible embedding:\n",
        "        # FastText sentence embedding:\n",
        "        #get_fastText_sentence_embedding_matrix(cleaned_tweets)\n",
        "        # only trained on tweets.csv:\n",
        "        get_fastText_gensim_embedding_matrix(cleaned_tweets)\n",
        "        # pretrained\n",
        "        get_fastText_embedding_matrix(cleaned_tweets)\n",
        "\n",
        "\n",
        "    elif mode == \"describe_dataset\":\n",
        "        # # Describe dataset with some common characteristics\n",
        "        describe_dataset(cleaned_tweets,hashtags)\n",
        "\n",
        "        # ## Plot eigenspectrum of embeddings\n",
        "        #data/\n",
        "        X = np.load(\"sentqs_skipgram_sentence_embedding.npz\",allow_pickle=True)\n",
        "        print(X.files)\n",
        "        \n",
        "        plot_eigenspectrum(X['embedding'])\n",
        "\n",
        "        # ## Plot representation of 2 dimensional tsne embedding\n",
        "        plot_tsne(X['embedding'],sentiment)\n",
        "\n",
        "    else:\n",
        "        ## Loads the data into the program and trains machine learning model\n",
        "        load_data_run_classification()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Obtain the all files of the dataset preprocessing, including plots, feature representation etc.\n",
        "    # After running this file you will find the corresponding files for classification in the data folder\n",
        "    # select mode:\n",
        "    main_preprocessing(\"multi_semantic_embedding\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seperate Tweets \n",
            "\n",
            "Found 73561 unique tokens.\n",
            "Shape of data tensor: (61529, 335)\n",
            "Shape of label tensor: (61529, 3)\n",
            "indices:  (61529,)\n",
            "data:  (61529, 335)\n",
            "Vocab size 73562\n",
            "Indexing word vectors.\n",
            "Found 1193514 word vectors.\n",
            "Preparing embedding matrix.\n",
            "Loaded Skipgram embedding.\n",
            "Vocab size 73562\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 335)]             0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 335, 200)          14712400  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 331, 64)           64064     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 82, 64)            0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 70)                37800     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 35)                2485      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 36        \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 1)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 14,816,791\n",
            "Trainable params: 104,391\n",
            "Non-trainable params: 14,712,400\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "669/669 [==============================] - 12s 18ms/step - loss: 1.0122 - acc: 0.5199 - val_loss: 0.9700 - val_acc: 0.5314\n",
            "Epoch 2/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.9414 - acc: 0.5389 - val_loss: 0.9295 - val_acc: 0.5459\n",
            "Epoch 3/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.9048 - acc: 0.5553 - val_loss: 0.9040 - val_acc: 0.5524\n",
            "Epoch 4/50\n",
            "669/669 [==============================] - 12s 17ms/step - loss: 0.8767 - acc: 0.5650 - val_loss: 0.9037 - val_acc: 0.5516\n",
            "Epoch 5/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.8526 - acc: 0.5740 - val_loss: 0.8881 - val_acc: 0.5574\n",
            "Epoch 6/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.8317 - acc: 0.5832 - val_loss: 0.8765 - val_acc: 0.5615\n",
            "Epoch 7/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.8088 - acc: 0.5907 - val_loss: 0.8727 - val_acc: 0.5606\n",
            "Epoch 8/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.7868 - acc: 0.5984 - val_loss: 0.8720 - val_acc: 0.5623\n",
            "Epoch 9/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.7703 - acc: 0.6072 - val_loss: 0.8716 - val_acc: 0.5649\n",
            "Epoch 10/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.7532 - acc: 0.6160 - val_loss: 0.8708 - val_acc: 0.5747\n",
            "Epoch 11/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.7346 - acc: 0.6333 - val_loss: 0.8625 - val_acc: 0.5954\n",
            "Epoch 12/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.7123 - acc: 0.6618 - val_loss: 0.8463 - val_acc: 0.6129\n",
            "Epoch 13/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.6794 - acc: 0.7031 - val_loss: 0.8402 - val_acc: 0.6326\n",
            "Epoch 14/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.6508 - acc: 0.7419 - val_loss: 0.8436 - val_acc: 0.6584\n",
            "Epoch 15/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.6215 - acc: 0.7662 - val_loss: 0.8466 - val_acc: 0.6511\n",
            "Epoch 16/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.5911 - acc: 0.7910 - val_loss: 0.8289 - val_acc: 0.6834\n",
            "Epoch 17/50\n",
            "669/669 [==============================] - 12s 17ms/step - loss: 0.5613 - acc: 0.8141 - val_loss: 0.8384 - val_acc: 0.6727\n",
            "Epoch 18/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.5349 - acc: 0.8278 - val_loss: 0.8319 - val_acc: 0.6896\n",
            "Epoch 19/50\n",
            "669/669 [==============================] - 12s 18ms/step - loss: 0.5102 - acc: 0.8431 - val_loss: 0.8513 - val_acc: 0.6911\n",
            "Epoch 20/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.4889 - acc: 0.8528 - val_loss: 0.8494 - val_acc: 0.6931\n",
            "Epoch 21/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.4668 - acc: 0.8634 - val_loss: 0.8861 - val_acc: 0.6933\n",
            "Epoch 22/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.4476 - acc: 0.8735 - val_loss: 0.8787 - val_acc: 0.6982\n",
            "Epoch 23/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.4263 - acc: 0.8812 - val_loss: 0.8926 - val_acc: 0.7012\n",
            "Epoch 24/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.4126 - acc: 0.8869 - val_loss: 0.9034 - val_acc: 0.6998\n",
            "Epoch 25/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3963 - acc: 0.8921 - val_loss: 0.9063 - val_acc: 0.7046\n",
            "Epoch 26/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3831 - acc: 0.8981 - val_loss: 0.9121 - val_acc: 0.7066\n",
            "Epoch 27/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3681 - acc: 0.9046 - val_loss: 0.9338 - val_acc: 0.7022\n",
            "Epoch 28/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3546 - acc: 0.9072 - val_loss: 0.9575 - val_acc: 0.6972\n",
            "Epoch 29/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3440 - acc: 0.9113 - val_loss: 0.9640 - val_acc: 0.7050\n",
            "Epoch 30/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3309 - acc: 0.9163 - val_loss: 0.9866 - val_acc: 0.7026\n",
            "Epoch 31/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3244 - acc: 0.9186 - val_loss: 1.0031 - val_acc: 0.7010\n",
            "Epoch 32/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3124 - acc: 0.9224 - val_loss: 0.9998 - val_acc: 0.7049\n",
            "Epoch 33/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.3011 - acc: 0.9263 - val_loss: 1.0630 - val_acc: 0.7028\n",
            "Epoch 34/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2961 - acc: 0.9258 - val_loss: 1.0227 - val_acc: 0.7031\n",
            "Epoch 35/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2854 - acc: 0.9308 - val_loss: 1.0756 - val_acc: 0.7009\n",
            "Epoch 36/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2754 - acc: 0.9332 - val_loss: 1.0708 - val_acc: 0.7050\n",
            "Epoch 37/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2696 - acc: 0.9343 - val_loss: 1.1383 - val_acc: 0.7002\n",
            "Epoch 38/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2614 - acc: 0.9363 - val_loss: 1.1098 - val_acc: 0.7057\n",
            "Epoch 39/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2568 - acc: 0.9376 - val_loss: 1.1651 - val_acc: 0.7013\n",
            "Epoch 40/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2484 - acc: 0.9398 - val_loss: 1.1579 - val_acc: 0.7035\n",
            "Epoch 41/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2427 - acc: 0.9413 - val_loss: 1.1473 - val_acc: 0.7023\n",
            "Epoch 42/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2366 - acc: 0.9407 - val_loss: 1.1982 - val_acc: 0.6973\n",
            "Epoch 43/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2299 - acc: 0.9439 - val_loss: 1.1755 - val_acc: 0.6984\n",
            "Epoch 44/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2216 - acc: 0.9470 - val_loss: 1.3411 - val_acc: 0.6832\n",
            "Epoch 45/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2178 - acc: 0.9478 - val_loss: 1.2771 - val_acc: 0.6976\n",
            "Epoch 46/50\n",
            "669/669 [==============================] - 12s 17ms/step - loss: 0.2141 - acc: 0.9482 - val_loss: 1.2294 - val_acc: 0.7062\n",
            "Epoch 47/50\n",
            "669/669 [==============================] - 12s 17ms/step - loss: 0.2092 - acc: 0.9500 - val_loss: 1.2257 - val_acc: 0.6991\n",
            "Epoch 48/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.2057 - acc: 0.9492 - val_loss: 1.2414 - val_acc: 0.7044\n",
            "Epoch 49/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.1984 - acc: 0.9536 - val_loss: 1.2488 - val_acc: 0.6993\n",
            "Epoch 50/50\n",
            "669/669 [==============================] - 11s 17ms/step - loss: 0.1962 - acc: 0.9523 - val_loss: 1.2824 - val_acc: 0.7040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}